{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLz0ql-Mcyw6"
   },
   "source": [
    "##############################################\n",
    "\n",
    "############# TABLE OF CONTENTS #############\n",
    "\n",
    "##############################################\n",
    "- 1) Import packages and functions\n",
    "- 2) Function for preprocessing the data\n",
    "- 3) Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2bwY9DoevNKF"
   },
   "outputs": [],
   "source": [
    "incomplete_levels = ['75']\n",
    "\n",
    "levels = {'75': 0.75}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DEILVF4O6Ec_"
   },
   "outputs": [],
   "source": [
    "import pu_xgboost_scikit as xgb_pu\n",
    "\n",
    "from scipy.special import expit\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0yRQ_ZSMaFG"
   },
   "source": [
    "# **import packages and functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uwBik9AUO-VZ"
   },
   "outputs": [],
   "source": [
    "# functions and packages\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import roc_curve\n",
    "import random\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "# import packages\n",
    "# packages from https://github.com/irhete/predictive-monitoring-benchmark/blob/master/experiments/experiments.py\n",
    "\n",
    "import EncoderFactory\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ud8f9rgWcyxI"
   },
   "source": [
    "## Functions from stackoverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uH3LRIBHOyap"
   },
   "outputs": [],
   "source": [
    "def transform_data(dt_train, dt_test, y_train):\n",
    "    # feature combiner and columns\n",
    "    feature_combiner = FeatureUnion([(method, EncoderFactory.get_encoder(\n",
    "          method, **cls_encoder_args)) for method in methods])\n",
    "    feature_combiner.fit(dt_train, y_train)\n",
    "\n",
    "    # transform train dataset and add the column names back to the dataframe\n",
    "    train_named = feature_combiner.transform(dt_train)\n",
    "    train_named = pd.DataFrame(train_named)\n",
    "    names = feature_combiner.get_feature_names()\n",
    "    train_named.columns = names\n",
    "\n",
    "    # transform test dataset\n",
    "    test_named = feature_combiner.transform(dt_test)\n",
    "    test_named = pd.DataFrame(test_named)\n",
    "    names = feature_combiner.get_feature_names()\n",
    "    test_named.columns = names\n",
    "\n",
    "    return train_named, test_named"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkzXHGIWjOVO"
   },
   "source": [
    "# Function to flip labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "u8xAQSa9jNub"
   },
   "outputs": [],
   "source": [
    "def count_labels(data_y):\n",
    "    print(\"total size\", len(data_y))\n",
    "    #print(\"regular\", data_y.count(\"regular\"))\n",
    "    #print(\"deviant\", data_y.count(\"deviant\"))\n",
    "\n",
    "def count_labels_number(data_y):\n",
    "    print(\"total size\", len(data_y))\n",
    "    #print(\"regular\", data_y.count(0))\n",
    "    #print(\"deviant\", data_y.count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "owiiRk-gvKlr"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import dataset_confs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "class DatasetManager:\n",
    "    \n",
    "    def __init__(self, dataset_name):\n",
    "        self.dataset_name = dataset_name\n",
    "        \n",
    "        self.case_id_col = dataset_confs.case_id_col[self.dataset_name]\n",
    "        self.activity_col = dataset_confs.activity_col[self.dataset_name]\n",
    "        self.timestamp_col = dataset_confs.timestamp_col[self.dataset_name]\n",
    "        self.label_col = dataset_confs.label_col[self.dataset_name]\n",
    "        self.pos_label = dataset_confs.pos_label[self.dataset_name]\n",
    "\n",
    "        self.dynamic_cat_cols = dataset_confs.dynamic_cat_cols[self.dataset_name]\n",
    "        self.static_cat_cols = dataset_confs.static_cat_cols[self.dataset_name]\n",
    "        self.dynamic_num_cols = dataset_confs.dynamic_num_cols[self.dataset_name]\n",
    "        self.static_num_cols = dataset_confs.static_num_cols[self.dataset_name]\n",
    "        \n",
    "        self.sorting_cols = [self.timestamp_col, self.activity_col]\n",
    "\n",
    "    \n",
    "    def read_dataset(self, datalocation):\n",
    "        # read dataset\n",
    "        dtypes = {col:\"object\" for col in self.dynamic_cat_cols+self.static_cat_cols+[self.case_id_col, self.label_col, self.timestamp_col]}\n",
    "        for col in self.dynamic_num_cols + self.static_num_cols:\n",
    "            dtypes[col] = \"float\"\n",
    "\n",
    "        data = pd.read_csv(datalocation, sep=\";\", dtype=dtypes)\n",
    "        data[self.timestamp_col] = pd.to_datetime(data[self.timestamp_col])\n",
    "\n",
    "        if self.dataset_name in ['bpic2011_f1', 'bpic2011_f2', 'bpic2011_f3', 'bpic2011_f4','bpic2015_1_f2','bpic2015_2_f2','bpic2015_3_f2','bpic2015_4_f2','bpic2015_5_f2','sepsis_cases_1','sepsis_cases_2','sepsis_cases_4']:\n",
    "            data['time:timestamp'] = pd.to_datetime(data['time:timestamp']) \n",
    "        if self.dataset_name in ['bpic2012_accepted', 'bpic2012_cancelled', 'bpic2012_declined']:\n",
    "            data['Complete Timestamp'] = pd.to_datetime(data['Complete Timestamp'])\n",
    "\n",
    "        return data\n",
    "    \n",
    "\n",
    "\n",
    "    def split_data(self, data, train_ratio, split=\"temporal\", seed=22):  \n",
    "        # split into train and test using temporal split\n",
    "\n",
    "        grouped = data.groupby(self.case_id_col)\n",
    "        start_timestamps = grouped[self.timestamp_col].min().reset_index()\n",
    "        if split == \"temporal\":\n",
    "            start_timestamps = start_timestamps.sort_values(self.timestamp_col, ascending=True, kind=\"mergesort\")\n",
    "        elif split == \"random\":\n",
    "            np.random.seed(seed)\n",
    "            start_timestamps = start_timestamps.reindex(np.random.permutation(start_timestamps.index))\n",
    "        train_ids = list(start_timestamps[self.case_id_col])[:int(train_ratio*len(start_timestamps))]\n",
    "        train = data[data[self.case_id_col].isin(train_ids)].sort_values(self.timestamp_col, ascending=True, kind='mergesort')\n",
    "        test = data[~data[self.case_id_col].isin(train_ids)].sort_values(self.timestamp_col, ascending=True, kind='mergesort')\n",
    "\n",
    "        return (train, test)\n",
    "    \n",
    "    def split_data_strict(self, data, train_ratio, split=\"temporal\"):  \n",
    "        # split into train and test using temporal split and discard events that overlap the periods\n",
    "        data = data.sort_values(self.sorting_cols, ascending=True, kind='mergesort')\n",
    "        grouped = data.groupby(self.case_id_col)\n",
    "        start_timestamps = grouped[self.timestamp_col].min().reset_index()\n",
    "        start_timestamps = start_timestamps.sort_values(self.timestamp_col, ascending=True, kind='mergesort')\n",
    "        train_ids = list(start_timestamps[self.case_id_col])[:int(train_ratio*len(start_timestamps))]\n",
    "        train = data[data[self.case_id_col].isin(train_ids)].sort_values(self.sorting_cols, ascending=True, kind='mergesort')\n",
    "        test = data[~data[self.case_id_col].isin(train_ids)].sort_values(self.sorting_cols, ascending=True, kind='mergesort')\n",
    "        split_ts = test[self.timestamp_col].min()\n",
    "        train = train[train[self.timestamp_col] < split_ts]\n",
    "        return (train, test)\n",
    "    \n",
    "    def split_data_discard(self, data, train_ratio, split=\"temporal\"):  \n",
    "        # split into train and test using temporal split and discard events that overlap the periods\n",
    "        data = data.sort_values(self.sorting_cols, ascending=True, kind='mergesort')\n",
    "        grouped = data.groupby(self.case_id_col)\n",
    "        start_timestamps = grouped[self.timestamp_col].min().reset_index()\n",
    "        start_timestamps = start_timestamps.sort_values(self.timestamp_col, ascending=True, kind='mergesort')\n",
    "        train_ids = list(start_timestamps[self.case_id_col])[:int(train_ratio*len(start_timestamps))]\n",
    "        train = data[data[self.case_id_col].isin(train_ids)].sort_values(self.sorting_cols, ascending=True, kind='mergesort')\n",
    "        test = data[~data[self.case_id_col].isin(train_ids)].sort_values(self.sorting_cols, ascending=True, kind='mergesort')\n",
    "        split_ts = test[self.timestamp_col].min()\n",
    "        overlapping_cases = train[train[self.timestamp_col] >= split_ts][self.case_id_col].unique()\n",
    "        train = train[~train[self.case_id_col].isin(overlapping_cases)]\n",
    "        return (train, test)\n",
    "    \n",
    "    \n",
    "    def split_val(self, data, val_ratio, split=\"random\", seed=22):  \n",
    "        # split into train and test using temporal split\n",
    "        grouped = data.groupby(self.case_id_col)\n",
    "        start_timestamps = grouped[self.timestamp_col].min().reset_index()\n",
    "        if split == \"temporal\":\n",
    "            start_timestamps = start_timestamps.sort_values(self.timestamp_col, ascending=True, kind=\"mergesort\")\n",
    "        elif split == \"random\":\n",
    "            np.random.seed(seed)\n",
    "            start_timestamps = start_timestamps.reindex(np.random.permutation(start_timestamps.index))\n",
    "        val_ids = list(start_timestamps[self.case_id_col])[-int(val_ratio*len(start_timestamps)):]\n",
    "        val = data[data[self.case_id_col].isin(val_ids)].sort_values(self.sorting_cols, ascending=True, kind=\"mergesort\")\n",
    "        train = data[~data[self.case_id_col].isin(val_ids)].sort_values(self.sorting_cols, ascending=True, kind=\"mergesort\")\n",
    "        return (train, val)\n",
    "\n",
    "\n",
    "    def generate_prefix_data(self, data, min_length, max_length, gap=1):\n",
    "        # generate prefix data (each possible prefix becomes a trace)\n",
    "        data['case_length'] = data.groupby(self.case_id_col)[self.activity_col].transform(len)\n",
    "\n",
    "        dt_prefixes = data[data['case_length'] >= min_length].groupby(self.case_id_col).head(min_length)\n",
    "        dt_prefixes[\"prefix_nr\"] = 1\n",
    "        dt_prefixes[\"orig_case_id\"] = dt_prefixes[self.case_id_col]\n",
    "        for nr_events in range(min_length+gap, max_length+1, gap):\n",
    "            tmp = data[data['case_length'] >= nr_events].groupby(self.case_id_col).head(nr_events)\n",
    "            tmp[\"orig_case_id\"] = tmp[self.case_id_col]\n",
    "            tmp[self.case_id_col] = tmp[self.case_id_col].apply(lambda x: \"%s_%s\"%(x, nr_events))\n",
    "            tmp[\"prefix_nr\"] = nr_events\n",
    "            dt_prefixes = pd.concat([dt_prefixes, tmp], axis=0)\n",
    "        \n",
    "        dt_prefixes['case_length'] = dt_prefixes['case_length'].apply(lambda x: min(max_length, x))\n",
    "        \n",
    "        return dt_prefixes\n",
    "\n",
    "\n",
    "    def get_pos_case_length_quantile(self, data, quantile=0.90):\n",
    "        return int(np.ceil(data[data[self.label_col]==self.pos_label].groupby(self.case_id_col).size().quantile(quantile)))\n",
    "\n",
    "    def get_indexes(self, data):\n",
    "        return data.groupby(self.case_id_col).first().index\n",
    "\n",
    "    def get_relevant_data_by_indexes(self, data, indexes):\n",
    "        return data[data[self.case_id_col].isin(indexes)]\n",
    "\n",
    "    def get_label(self, data):\n",
    "        return data.groupby(self.case_id_col).first()[self.label_col]\n",
    "    \n",
    "    def get_prefix_lengths(self, data):\n",
    "        return data.groupby(self.case_id_col).last()[\"prefix_nr\"]\n",
    "    \n",
    "    def get_case_ids(self, data, nr_events=1):\n",
    "        case_ids = pd.Series(data.groupby(self.case_id_col).first().index)\n",
    "        if nr_events > 1:\n",
    "            case_ids = case_ids.apply(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n",
    "        return case_ids\n",
    "    \n",
    "    def get_label_numeric(self, data):\n",
    "        y = self.get_label(data) # one row per case\n",
    "        return [1 if label == self.pos_label else 0 for label in y]\n",
    "    \n",
    "    def get_class_ratio(self, data):\n",
    "        class_freqs = data[self.label_col].value_counts()\n",
    "        return class_freqs[self.pos_label] / class_freqs.sum()\n",
    "    \n",
    "    def get_stratified_split_generator(self, data, n_splits=5, shuffle=True, random_state=22):\n",
    "        grouped_firsts = data.groupby(self.case_id_col, as_index=False).first()\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "        \n",
    "        for train_index, test_index in skf.split(grouped_firsts, grouped_firsts[self.label_col]):\n",
    "            current_train_names = grouped_firsts[self.case_id_col][train_index]\n",
    "            train_chunk = data[data[self.case_id_col].isin(current_train_names)].sort_values(self.timestamp_col, ascending=True, kind='mergesort')\n",
    "            test_chunk = data[~data[self.case_id_col].isin(current_train_names)].sort_values(self.timestamp_col, ascending=True, kind='mergesort')\n",
    "            yield (train_chunk, test_chunk)\n",
    "            \n",
    "    def get_idx_split_generator(self, dt_for_splitting, n_splits=5, shuffle=True, random_state=22):\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "        \n",
    "        for train_index, test_index in skf.split(dt_for_splitting, dt_for_splitting[self.label_col]):\n",
    "            current_train_names = dt_for_splitting[self.case_id_col][train_index]\n",
    "            current_test_names = dt_for_splitting[self.case_id_col][test_index]\n",
    "            yield (current_train_names, current_test_names)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcKLuejWcyxN"
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ufibRgR8cyxO"
   },
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "params_dir = 'params'\n",
    "results_dir ='results'\n",
    "column_selection = 'all'\n",
    "train_ratio = 0.8\n",
    "n_splits = 3\n",
    "random_state = 22\n",
    "n_iter = 1\n",
    "\n",
    "# create results directory\n",
    "if not os.path.exists(os.path.join(results_dir)):\n",
    "    os.makedirs(os.path.join(results_dir))\n",
    "\n",
    "encoding_dict = {\n",
    "    \"agg\": [\"static\", \"agg\"],\n",
    "    # \"index\": [\"static\", \"index\"]\n",
    "}\n",
    "encoding = []\n",
    "for k, v in encoding_dict.items():\n",
    "    encoding.append(k)\n",
    "\n",
    "csv_files = {\n",
    "    #\"bpic2011\": [\"BPIC11_f%s\"%formula for formula in range(4,5)],\n",
    "    \"bpic2015\": [\"BPIC15_%s_f2\"%(municipality) for municipality in range(1,5)],\n",
    "    #\"sepsis_cases\": [\"sepsis_cases_4\"],\n",
    "    #\"bpic2012\": [\"bpic2012_O_ACCEPTED#COMPLETE\",\"bpic2012_O_CANCELLED-COMPLETE\",\"bpic2012_0_DECLINED-COMPLETE\"],\n",
    "    #production\": [\"Production\"],\n",
    "    #\"bpic2017\": [\"BPIC17_O_Accepted\",\"BPIC17_O_Cancelled\",\"BPIC17_0_Refused\"],\n",
    "    #\"bpic2017\": [\"BPIC17_O_Cancelled\"],\n",
    "    #\"traffic_fines\": [\"traffic_fines_%s\"%formula for formula in range(1,3)],\n",
    "    #\"hospital_billing\": [\"hospital_billing_%s\"%suffix for suffix in [2,3]]\n",
    "}\n",
    "files = []\n",
    "for k, v in csv_files.items():\n",
    "    files.extend(v)\n",
    "dataset_ref_to_datasets = {\n",
    "    #\"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(4,5)],\n",
    "    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(1,5)],\n",
    "    #\"sepsis_cases\": [\"sepsis_cases_4\"]\n",
    "    #\"bpic2012\": [\"bpic2012_accepted\",\"bpic2012_cancelled\",\"bpic2012_declined\"],\n",
    "    #\"production\": [\"production\"],\n",
    "    #\"bpic2017\": [\"bpic2017_cancelled\"],\n",
    "    #\"bpic2017\": [\"bpic2017_accepted\",\"bpic2017_cancelled\",\"bpic2017_refused\"],\n",
    "    #\"traffic_fines\": [\"traffic_fines_%s\"%formula for formula in range(1,3)],\n",
    "    #\"hospital_billing\": [\"hospital_billing_%s\"%suffix for suffix in [2,3]]\n",
    "}\n",
    "\n",
    "\n",
    "files = []\n",
    "for k, v in csv_files.items():\n",
    "    files.extend(v)\n",
    "datasets = []\n",
    "for k, v in dataset_ref_to_datasets.items():\n",
    "    datasets.extend(v)\n",
    "res = {datasets[i]: files[i] for i in range(len(datasets))}\n",
    "\n",
    "# classifiers dictionary\n",
    "classifier_ref_to_classifiers = {\n",
    "     \"MLmodels\": [\"XGB\"],\n",
    "   }\n",
    "classifiers = []\n",
    "for k, v in classifier_ref_to_classifiers.items():\n",
    "    classifiers.extend(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVX1WhP2Mvv-"
   },
   "source": [
    "# **loop over datasets and classifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "agyg5xxq09yo",
    "outputId": "f016d6d7-ce0e-4922-c75a-772fffcf6a84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: bpic2015_1_f2\n",
      "Classifier XGB\n",
      "Encoding agg\n",
      "{'colsample_bytree': 0.888255940141602, 'learning_rate': 0.05242434039476451, 'max_depth': 18, 'min_child_weight': 3, 'subsample': 0.5978091736611192}\n",
      "total size 18345\n",
      "total size 4876\n",
      "0.6311816329167386\n",
      "Dataset: bpic2015_2_f2\n",
      "Classifier XGB\n",
      "Encoding agg\n",
      "{'colsample_bytree': 0.6501254795018532, 'learning_rate': 0.6156269335682346, 'max_depth': 26, 'min_child_weight': 4, 'subsample': 0.9998530424055201}\n",
      "total size 22221\n",
      "total size 5789\n",
      "0.8580695568676378\n",
      "Dataset: bpic2015_3_f2\n",
      "Classifier XGB\n",
      "Encoding agg\n",
      "{'colsample_bytree': 0.5362998444273354, 'learning_rate': 0.22310535761696237, 'max_depth': 29, 'min_child_weight': 1, 'subsample': 0.8702717321620221}\n",
      "total size 37400\n",
      "total size 10041\n",
      "0.8973726687942919\n",
      "Dataset: bpic2015_4_f2\n",
      "Classifier XGB\n",
      "Encoding agg\n",
      "{'colsample_bytree': 0.7596047324701893, 'learning_rate': 0.05489611091934121, 'max_depth': 23, 'min_child_weight': 4, 'subsample': 0.6200715735743001}\n",
      "total size 16261\n",
      "total size 3970\n",
      "0.8345703996149141\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in datasets:\n",
    "    for cls_method in classifiers:\n",
    "        for cls_encoding in encoding:\n",
    "            for level in incomplete_levels: \n",
    "                print('Dataset:', dataset_name)\n",
    "                print('Classifier', cls_method)\n",
    "                print('Encoding', cls_encoding)\n",
    "                dataset_manager = DatasetManager(dataset_name)\n",
    "                dataset_name_csv = res[dataset_name]\n",
    "                data = dataset_manager.read_dataset('Original_data/'+dataset_name_csv+'.csv')\n",
    "                dataset_name_csv = res[dataset_name]\n",
    "                method_name = \"%s_%s\" % (column_selection, cls_encoding)\n",
    "                methods = encoding_dict[cls_encoding]\n",
    "\n",
    "                # extract the optimal parameters\n",
    "                optimal_params_filename = os.path.join(params_dir,'PU_optimal_params_%s_%s_%s_%s.pickle' % (cls_method, dataset_name, level, method_name))\n",
    "                if not os.path.isfile(optimal_params_filename) or os.path.getsize(optimal_params_filename) <= 0:\n",
    "                    print('problem')\n",
    "                with open(optimal_params_filename, \"rb\") as fin:\n",
    "                    args = pickle.load(fin)\n",
    "                    print(args)\n",
    "\n",
    "                cls_encoder_args = {'case_id_col': dataset_manager.case_id_col,\n",
    "                                    'static_cat_cols': dataset_manager.static_cat_cols,\n",
    "                                    'static_num_cols': dataset_manager.static_num_cols,\n",
    "                                    'dynamic_cat_cols': dataset_manager.dynamic_cat_cols,\n",
    "                                    'dynamic_num_cols': dataset_manager.dynamic_num_cols,\n",
    "                                    'fillna': True}\n",
    "\n",
    "                #file to save results\n",
    "                outfile = os.path.join(results_dir, \"PU_performance_results_%s_%s_%s_%s.csv\" % (cls_method, dataset_name, level, method_name))\n",
    "\n",
    "                # determine min and max (truncated) prefix lengths\n",
    "                min_prefix_length = 1\n",
    "                if \"traffic_fines\" in dataset_name:\n",
    "                    max_prefix_length = 10\n",
    "                elif \"bpic2017\" in dataset_name:\n",
    "                    max_prefix_length = min(20, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n",
    "                else:\n",
    "                    max_prefix_length = min(40, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n",
    "\n",
    "                maxlen = cutoff = max_prefix_length \n",
    "\n",
    "                # split into training and test\n",
    "                train = dataset_manager.read_dataset('Data/Train_PU'+level+'_'+dataset_name_csv+'.csv')\n",
    "                test = dataset_manager.read_dataset('Data/Test_'+dataset_name_csv+'.csv')\n",
    "                #prefix generation of train and test data\n",
    "                dt_train_prefixes = dataset_manager.generate_prefix_data(train, min_prefix_length, max_prefix_length)\n",
    "                dt_test_prefixes = dataset_manager.generate_prefix_data(test, min_prefix_length, max_prefix_length)\n",
    "                test_y = dataset_manager.get_label_numeric(dt_test_prefixes)\n",
    "                train_y = dataset_manager.get_label_numeric(dt_train_prefixes)\n",
    "                dt_train_named, dt_test_named = transform_data(dt_train_prefixes, dt_test_prefixes, train_y)\n",
    "\n",
    "                #DELETE THIS LATER\n",
    "                count_labels_number(train_y)\n",
    "                count_labels_number(test_y)\n",
    "\n",
    "                #create the input layers and embeddings\n",
    "                embeddings= []\n",
    "                input_layers = []\n",
    "                preds_all = []\n",
    "                nr_events_all = []\n",
    "                nr_events = list(dataset_manager.get_prefix_lengths(dt_test_prefixes))\n",
    "                nr_events_all.extend(nr_events)\n",
    "                test_y_all = []\n",
    "                test_y_all.extend(test_y)\n",
    "                #MODEL\n",
    "                flip_ratio_ = levels[level]  \n",
    "                label_freq_ = 1.0 - flip_ratio_  ## P(labeled | y = 1)\n",
    "\n",
    "\n",
    "\n",
    "                current_args = args\n",
    "                cls = xgb_pu.PUBoost(obj='nnpu',\n",
    "                                     n_estimators=500,\n",
    "                                     learning_rate= current_args['learning_rate'],\n",
    "                                     subsample=current_args['subsample'],\n",
    "                                     max_depth=int(current_args['max_depth']),\n",
    "                                     colsample_bytree=current_args['colsample_bytree'],\n",
    "                                     min_child_weight=int(current_args['min_child_weight']),\n",
    "                                     random_state=random_state,\n",
    "                                     label_freq = label_freq_).fit(dt_train_named,train_y)\n",
    "\n",
    "                # predictions\n",
    "                pred= expit(cls.inplace_predict(dt_test_named))\n",
    "                preds_all.extend(pred)\n",
    "                auc_total = roc_auc_score(test_y_all, preds_all)\n",
    "\n",
    "\n",
    "                score = 0\n",
    "                dim = 0\n",
    "                auc_total = roc_auc_score(test_y_all, preds_all)\n",
    "\n",
    "                print(auc_total)\n",
    "                with open(outfile, 'w') as fout:\n",
    "                    fout.write(\"%s;%s;%s;%s;%s;%s\\n\" % (\"dataset\", \"method\", \"cls\", \"nr_events\", \"metric\", \"score\")) \n",
    "                    dt_results = pd.DataFrame({\"actual\": test_y_all, \"predicted\": preds_all, \"nr_events\": nr_events_all})\n",
    "                    for nr_events, group in dt_results.groupby(\"nr_events\"):\n",
    "                        if len(set(group.actual)) < 2:\n",
    "                            fout.write(\"%s;%s;%s;%s;%s;%s;%s\\n\" % (dataset_name, method_name, cls_method, nr_events, -1,\n",
    "                                                                   \"auc\", np.nan))\n",
    "                        else:\n",
    "                            fout.write(\"%s;%s;%s;%s;%s;%s;%s\\n\" % (dataset_name, method_name, cls_method, nr_events, -1,\n",
    "                                                                   \"auc\", roc_auc_score(group.actual, group.predicted)))\n",
    "                    fout.write(\"%s;%s;%s;%s;%s;%s\\n\" % (dataset_name, method_name, cls_method, -1, \"auc\",\n",
    "                                                        roc_auc_score(dt_results.actual, dt_results.predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T4SCL-P4E2dt",
    "outputId": "db337b65-70c3-4911-c2b6-af626285505e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.7596047324701893,\n",
       " 'learning_rate': 0.05489611091934121,\n",
       " 'max_depth': 23,\n",
       " 'min_child_weight': 4,\n",
       " 'subsample': 0.6200715735743001}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MjW9RH8vE2du"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "v0yRQ_ZSMaFG",
    "UpCBthWhMhkS",
    "HcKLuejWcyxN"
   ],
   "machine_shape": "hm",
   "name": "PU_Experiments_XGB_(PU_OOPPM).ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
