{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bwY9DoevNKF"
   },
   "outputs": [],
   "source": [
    "incomplete_levels = ['25', '50', '75']\n",
    "\n",
    "levels = {'25': 0.25, '50': 0.50, '75': 0.75}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sa7KmzGxRW3"
   },
   "outputs": [],
   "source": [
    "import pu_xgboost_scikit as xgb_pu\n",
    "\n",
    "from scipy.special import expit\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0yRQ_ZSMaFG"
   },
   "source": [
    "# **import packages and functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uwBik9AUO-VZ"
   },
   "outputs": [],
   "source": [
    "# functions and packages\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import roc_curve\n",
    "import random\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "# import packages\n",
    "# packages from https://github.com/irhete/predictive-monitoring-benchmark/blob/master/experiments/experiments.py\n",
    "\n",
    "import EncoderFactory\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ud8f9rgWcyxI"
   },
   "source": [
    "## Functions from stackoverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uH3LRIBHOyap"
   },
   "outputs": [],
   "source": [
    "def transform_data(dt_train, dt_test, y_train):\n",
    "    # feature combiner and columns\n",
    "    feature_combiner = FeatureUnion([(method, EncoderFactory.get_encoder(\n",
    "          method, **cls_encoder_args)) for method in methods])\n",
    "    feature_combiner.fit(dt_train, y_train)\n",
    "\n",
    "    # transform train dataset and add the column names back to the dataframe\n",
    "    train_named = feature_combiner.transform(dt_train)\n",
    "    train_named = pd.DataFrame(train_named)\n",
    "    names = feature_combiner.get_feature_names()\n",
    "    train_named.columns = names\n",
    "\n",
    "    # transform test dataset\n",
    "    test_named = feature_combiner.transform(dt_test)\n",
    "    test_named = pd.DataFrame(test_named)\n",
    "    names = feature_combiner.get_feature_names()\n",
    "    test_named.columns = names\n",
    "\n",
    "    return train_named, test_named"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkzXHGIWjOVO"
   },
   "source": [
    "# Function to flip labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8xAQSa9jNub"
   },
   "outputs": [],
   "source": [
    "def count_labels(data_y):\n",
    "    print(\"total size\", len(data_y))\n",
    "    #print(\"regular\", data_y.count(\"regular\"))\n",
    "    #print(\"deviant\", data_y.count(\"deviant\"))\n",
    "\n",
    "def count_labels_number(data_y):\n",
    "    print(\"total size\", len(data_y))\n",
    "    #print(\"regular\", data_y.count(0))\n",
    "    #print(\"deviant\", data_y.count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owiiRk-gvKlr"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import dataset_confs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "class DatasetManager:\n",
    "    \n",
    "    def __init__(self, dataset_name):\n",
    "        self.dataset_name = dataset_name\n",
    "        \n",
    "        self.case_id_col = dataset_confs.case_id_col[self.dataset_name]\n",
    "        self.activity_col = dataset_confs.activity_col[self.dataset_name]\n",
    "        self.timestamp_col = dataset_confs.timestamp_col[self.dataset_name]\n",
    "        self.label_col = dataset_confs.label_col[self.dataset_name]\n",
    "        self.pos_label = dataset_confs.pos_label[self.dataset_name]\n",
    "\n",
    "        self.dynamic_cat_cols = dataset_confs.dynamic_cat_cols[self.dataset_name]\n",
    "        self.static_cat_cols = dataset_confs.static_cat_cols[self.dataset_name]\n",
    "        self.dynamic_num_cols = dataset_confs.dynamic_num_cols[self.dataset_name]\n",
    "        self.static_num_cols = dataset_confs.static_num_cols[self.dataset_name]\n",
    "        \n",
    "        self.sorting_cols = [self.timestamp_col, self.activity_col]\n",
    "\n",
    "    \n",
    "    def read_dataset(self, datalocation):\n",
    "        # read dataset\n",
    "        dtypes = {col:\"object\" for col in self.dynamic_cat_cols+self.static_cat_cols+[self.case_id_col, self.label_col, self.timestamp_col]}\n",
    "        for col in self.dynamic_num_cols + self.static_num_cols:\n",
    "            dtypes[col] = \"float\"\n",
    "\n",
    "        data = pd.read_csv(datalocation, sep=\";\", dtype=dtypes)\n",
    "        data[self.timestamp_col] = pd.to_datetime(data[self.timestamp_col])\n",
    "\n",
    "        if self.dataset_name in ['bpic2011_f1', 'bpic2011_f2', 'bpic2011_f3', 'bpic2011_f4','bpic2015_1_f2','bpic2015_2_f2','bpic2015_3_f2','bpic2015_4_f2','bpic2015_5_f2','sepsis_cases_1','sepsis_cases_2','sepsis_cases_4']:\n",
    "            data['time:timestamp'] = pd.to_datetime(data['time:timestamp']) \n",
    "        if self.dataset_name in ['bpic2012_accepted', 'bpic2012_cancelled', 'bpic2012_declined']:\n",
    "            data['Complete Timestamp'] = pd.to_datetime(data['Complete Timestamp'])\n",
    "\n",
    "        return data\n",
    "    \n",
    "\n",
    "\n",
    "    def split_data(self, data, train_ratio, split=\"temporal\", seed=22):  \n",
    "        # split into train and test using temporal split\n",
    "\n",
    "        grouped = data.groupby(self.case_id_col)\n",
    "        start_timestamps = grouped[self.timestamp_col].min().reset_index()\n",
    "        if split == \"temporal\":\n",
    "            start_timestamps = start_timestamps.sort_values(self.timestamp_col, ascending=True, kind=\"mergesort\")\n",
    "        elif split == \"random\":\n",
    "            np.random.seed(seed)\n",
    "            start_timestamps = start_timestamps.reindex(np.random.permutation(start_timestamps.index))\n",
    "        train_ids = list(start_timestamps[self.case_id_col])[:int(train_ratio*len(start_timestamps))]\n",
    "        train = data[data[self.case_id_col].isin(train_ids)].sort_values(self.timestamp_col, ascending=True, kind='mergesort')\n",
    "        test = data[~data[self.case_id_col].isin(train_ids)].sort_values(self.timestamp_col, ascending=True, kind='mergesort')\n",
    "\n",
    "        return (train, test)\n",
    "    \n",
    "    def split_data_strict(self, data, train_ratio, split=\"temporal\"):  \n",
    "        # split into train and test using temporal split and discard events that overlap the periods\n",
    "        data = data.sort_values(self.sorting_cols, ascending=True, kind='mergesort')\n",
    "        grouped = data.groupby(self.case_id_col)\n",
    "        start_timestamps = grouped[self.timestamp_col].min().reset_index()\n",
    "        start_timestamps = start_timestamps.sort_values(self.timestamp_col, ascending=True, kind='mergesort')\n",
    "        train_ids = list(start_timestamps[self.case_id_col])[:int(train_ratio*len(start_timestamps))]\n",
    "        train = data[data[self.case_id_col].isin(train_ids)].sort_values(self.sorting_cols, ascending=True, kind='mergesort')\n",
    "        test = data[~data[self.case_id_col].isin(train_ids)].sort_values(self.sorting_cols, ascending=True, kind='mergesort')\n",
    "        split_ts = test[self.timestamp_col].min()\n",
    "        train = train[train[self.timestamp_col] < split_ts]\n",
    "        return (train, test)\n",
    "    \n",
    "    def split_data_discard(self, data, train_ratio, split=\"temporal\"):  \n",
    "        # split into train and test using temporal split and discard events that overlap the periods\n",
    "        data = data.sort_values(self.sorting_cols, ascending=True, kind='mergesort')\n",
    "        grouped = data.groupby(self.case_id_col)\n",
    "        start_timestamps = grouped[self.timestamp_col].min().reset_index()\n",
    "        start_timestamps = start_timestamps.sort_values(self.timestamp_col, ascending=True, kind='mergesort')\n",
    "        train_ids = list(start_timestamps[self.case_id_col])[:int(train_ratio*len(start_timestamps))]\n",
    "        train = data[data[self.case_id_col].isin(train_ids)].sort_values(self.sorting_cols, ascending=True, kind='mergesort')\n",
    "        test = data[~data[self.case_id_col].isin(train_ids)].sort_values(self.sorting_cols, ascending=True, kind='mergesort')\n",
    "        split_ts = test[self.timestamp_col].min()\n",
    "        overlapping_cases = train[train[self.timestamp_col] >= split_ts][self.case_id_col].unique()\n",
    "        train = train[~train[self.case_id_col].isin(overlapping_cases)]\n",
    "        return (train, test)\n",
    "    \n",
    "    \n",
    "    def split_val(self, data, val_ratio, split=\"random\", seed=22):  \n",
    "        # split into train and test using temporal split\n",
    "        grouped = data.groupby(self.case_id_col)\n",
    "        start_timestamps = grouped[self.timestamp_col].min().reset_index()\n",
    "        if split == \"temporal\":\n",
    "            start_timestamps = start_timestamps.sort_values(self.timestamp_col, ascending=True, kind=\"mergesort\")\n",
    "        elif split == \"random\":\n",
    "            np.random.seed(seed)\n",
    "            start_timestamps = start_timestamps.reindex(np.random.permutation(start_timestamps.index))\n",
    "        val_ids = list(start_timestamps[self.case_id_col])[-int(val_ratio*len(start_timestamps)):]\n",
    "        val = data[data[self.case_id_col].isin(val_ids)].sort_values(self.sorting_cols, ascending=True, kind=\"mergesort\")\n",
    "        train = data[~data[self.case_id_col].isin(val_ids)].sort_values(self.sorting_cols, ascending=True, kind=\"mergesort\")\n",
    "        return (train, val)\n",
    "\n",
    "\n",
    "    def generate_prefix_data(self, data, min_length, max_length, gap=1):\n",
    "        # generate prefix data (each possible prefix becomes a trace)\n",
    "        data['case_length'] = data.groupby(self.case_id_col)[self.activity_col].transform(len)\n",
    "\n",
    "        dt_prefixes = data[data['case_length'] >= min_length].groupby(self.case_id_col).head(min_length)\n",
    "        dt_prefixes[\"prefix_nr\"] = 1\n",
    "        dt_prefixes[\"orig_case_id\"] = dt_prefixes[self.case_id_col]\n",
    "        for nr_events in range(min_length+gap, max_length+1, gap):\n",
    "            tmp = data[data['case_length'] >= nr_events].groupby(self.case_id_col).head(nr_events)\n",
    "            tmp[\"orig_case_id\"] = tmp[self.case_id_col]\n",
    "            tmp[self.case_id_col] = tmp[self.case_id_col].apply(lambda x: \"%s_%s\"%(x, nr_events))\n",
    "            tmp[\"prefix_nr\"] = nr_events\n",
    "            dt_prefixes = pd.concat([dt_prefixes, tmp], axis=0)\n",
    "        \n",
    "        dt_prefixes['case_length'] = dt_prefixes['case_length'].apply(lambda x: min(max_length, x))\n",
    "        \n",
    "        return dt_prefixes\n",
    "\n",
    "\n",
    "    def get_pos_case_length_quantile(self, data, quantile=0.90):\n",
    "        return int(np.ceil(data[data[self.label_col]==self.pos_label].groupby(self.case_id_col).size().quantile(quantile)))\n",
    "\n",
    "    def get_indexes(self, data):\n",
    "        return data.groupby(self.case_id_col).first().index\n",
    "\n",
    "    def get_relevant_data_by_indexes(self, data, indexes):\n",
    "        return data[data[self.case_id_col].isin(indexes)]\n",
    "\n",
    "    def get_label(self, data):\n",
    "        return data.groupby(self.case_id_col).first()[self.label_col]\n",
    "    \n",
    "    def get_prefix_lengths(self, data):\n",
    "        return data.groupby(self.case_id_col).last()[\"prefix_nr\"]\n",
    "    \n",
    "    def get_case_ids(self, data, nr_events=1):\n",
    "        case_ids = pd.Series(data.groupby(self.case_id_col).first().index)\n",
    "        if nr_events > 1:\n",
    "            case_ids = case_ids.apply(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n",
    "        return case_ids\n",
    "    \n",
    "    def get_label_numeric(self, data):\n",
    "        y = self.get_label(data) # one row per case\n",
    "        return [1 if label == self.pos_label else 0 for label in y]\n",
    "    \n",
    "    def get_class_ratio(self, data):\n",
    "        class_freqs = data[self.label_col].value_counts()\n",
    "        return class_freqs[self.pos_label] / class_freqs.sum()\n",
    "    \n",
    "    def get_stratified_split_generator(self, data, n_splits=5, shuffle=True, random_state=22):\n",
    "        grouped_firsts = data.groupby(self.case_id_col, as_index=False).first()\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "        \n",
    "        for train_index, test_index in skf.split(grouped_firsts, grouped_firsts[self.label_col]):\n",
    "            current_train_names = grouped_firsts[self.case_id_col][train_index]\n",
    "            train_chunk = data[data[self.case_id_col].isin(current_train_names)].sort_values(self.timestamp_col, ascending=True, kind='mergesort')\n",
    "            test_chunk = data[~data[self.case_id_col].isin(current_train_names)].sort_values(self.timestamp_col, ascending=True, kind='mergesort')\n",
    "            yield (train_chunk, test_chunk)\n",
    "            \n",
    "    def get_idx_split_generator(self, dt_for_splitting, n_splits=5, shuffle=True, random_state=22):\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "        \n",
    "        for train_index, test_index in skf.split(dt_for_splitting, dt_for_splitting[self.label_col]):\n",
    "            current_train_names = dt_for_splitting[self.case_id_col][train_index]\n",
    "            current_test_names = dt_for_splitting[self.case_id_col][test_index]\n",
    "            yield (current_train_names, current_test_names)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcKLuejWcyxN"
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufibRgR8cyxO"
   },
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "params_dir = 'params'\n",
    "results_dir ='results'\n",
    "column_selection = 'all'\n",
    "train_ratio = 0.8\n",
    "n_splits = 3\n",
    "random_state = 22\n",
    "n_iter = 1\n",
    "\n",
    "# create results directory\n",
    "if not os.path.exists(os.path.join(results_dir)):\n",
    "    os.makedirs(os.path.join(results_dir))\n",
    "\n",
    "encoding_dict = {\n",
    "    \"agg\": [\"static\", \"agg\"],\n",
    "    # \"index\": [\"static\", \"index\"]\n",
    "}\n",
    "encoding = []\n",
    "for k, v in encoding_dict.items():\n",
    "    encoding.append(k)\n",
    "\n",
    "csv_files = {\n",
    "    \"bpic2011\": [\"BPIC11_f%s\"%formula for formula in range(2,4)],\n",
    "    \"bpic2015\": [\"BPIC15_%s_f2\"%(municipality) for municipality in range(1,4)],\n",
    "    #\"sepsis_cases\": [\"sepsis_cases_4\"],\n",
    "    #\"bpic2012\": [\"bpic2012_O_ACCEPTED#COMPLETE\",\"bpic2012_O_CANCELLED-COMPLETE\",\"bpic2012_0_DECLINED-COMPLETE\"],\n",
    "    #production\": [\"Production\"],\n",
    "    #\"bpic2017\": [\"BPIC17_O_Accepted\",\"BPIC17_O_Cancelled\",\"BPIC17_0_Refused\"],\n",
    "    #\"bpic2017\": [\"BPIC17_O_Cancelled\"],\n",
    "    #\"traffic_fines\": [\"traffic_fines_%s\"%formula for formula in range(1,3)],\n",
    "    #\"hospital_billing\": [\"hospital_billing_%s\"%suffix for suffix in [2,3]]\n",
    "}\n",
    "files = []\n",
    "for k, v in csv_files.items():\n",
    "    files.extend(v)\n",
    "dataset_ref_to_datasets = {\n",
    "    \"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(2,4)],\n",
    "    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(1,4)],\n",
    "    #\"sepsis_cases\": [\"sepsis_cases_4\"]\n",
    "    #\"bpic2012\": [\"bpic2012_accepted\",\"bpic2012_cancelled\",\"bpic2012_declined\"],\n",
    "    #\"production\": [\"production\"],\n",
    "    #\"bpic2017\": [\"bpic2017_cancelled\"],\n",
    "    #\"bpic2017\": [\"bpic2017_accepted\",\"bpic2017_cancelled\",\"bpic2017_refused\"],\n",
    "    #\"traffic_fines\": [\"traffic_fines_%s\"%formula for formula in range(1,3)],\n",
    "    #\"hospital_billing\": [\"hospital_billing_%s\"%suffix for suffix in [2,3]]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "files = []\n",
    "for k, v in csv_files.items():\n",
    "    files.extend(v)\n",
    "datasets = []\n",
    "for k, v in dataset_ref_to_datasets.items():\n",
    "    datasets.extend(v)\n",
    "res = {datasets[i]: files[i] for i in range(len(datasets))}\n",
    "\n",
    "# classifiers dictionary\n",
    "classifier_ref_to_classifiers = {\n",
    "     \"MLmodels\": [\"XGB\"],\n",
    "   }\n",
    "classifiers = []\n",
    "for k, v in classifier_ref_to_classifiers.items():\n",
    "    classifiers.extend(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVX1WhP2Mvv-"
   },
   "source": [
    "# **loop over datasets and classifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "agyg5xxq09yo",
    "outputId": "aa64aef2-e7c4-4129-fd82-eb772237f645"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: bpic2011_f2\n",
      "Classifier XGB\n",
      "Encoding agg\n",
      "{'colsample_bytree': 0.5120815561823411, 'learning_rate': 0.08822713990469233, 'max_depth': 21, 'min_child_weight': 5, 'subsample': 0.6937950102363983}\n",
      "total size 22120\n",
      "total size 7545\n",
      "0.9214512453066036\n",
      "Dataset: bpic2011_f2\n",
      "Classifier XGB\n",
      "Encoding agg\n",
      "{'colsample_bytree': 0.968066060432367, 'learning_rate': 0.03293302605906889, 'max_depth': 20, 'min_child_weight': 3, 'subsample': 0.6721377628336107}\n",
      "total size 22120\n",
      "total size 7545\n",
      "0.9424349224863201\n",
      "Dataset: bpic2011_f2\n",
      "Classifier XGB\n",
      "Encoding agg\n",
      "{'colsample_bytree': 0.596712381998725, 'learning_rate': 0.555025757657036, 'max_depth': 21, 'min_child_weight': 2, 'subsample': 0.6481672640020901}\n",
      "total size 22120\n",
      "total size 7545\n",
      "0.5455033749166871\n",
      "Dataset: bpic2011_f3\n",
      "Classifier XGB\n",
      "Encoding agg\n",
      "{'colsample_bytree': 0.6027239376180052, 'learning_rate': 0.001835414400478852, 'max_depth': 9, 'min_child_weight': 1, 'subsample': 0.5735212476229243}\n",
      "total size 14669\n",
      "total size 4892\n",
      "0.9875644784825548\n",
      "Dataset: bpic2011_f3\n",
      "Classifier XGB\n",
      "Encoding agg\n",
      "{'colsample_bytree': 0.648860437284442, 'learning_rate': 0.6018321748511412, 'max_depth': 10, 'min_child_weight': 5, 'subsample': 0.6947717797539705}\n",
      "total size 14669\n",
      "total size 4892\n",
      "0.8318093968241757\n",
      "Dataset: bpic2011_f3\n",
      "Classifier XGB\n",
      "Encoding agg\n",
      "{'colsample_bytree': 0.6097306303004424, 'learning_rate': 0.28235070484376523, 'max_depth': 16, 'min_child_weight': 4, 'subsample': 0.9362637561137257}\n",
      "total size 14669\n",
      "total size 4892\n",
      "0.9119116006740765\n",
      "Dataset: bpic2015_1_f2\n",
      "Classifier XGB\n",
      "Encoding agg\n",
      "{'colsample_bytree': 0.9530213198010822, 'learning_rate': 0.09494073594544472, 'max_depth': 9, 'min_child_weight': 2, 'subsample': 0.7802119207095661}\n",
      "total size 18345\n",
      "total size 4876\n",
      "0.9177947947915344\n",
      "Dataset: bpic2015_1_f2\n",
      "Classifier XGB\n",
      "Encoding agg\n",
      "{'colsample_bytree': 0.9991833949305771, 'learning_rate': 0.23650240569238556, 'max_depth': 8, 'min_child_weight': 4, 'subsample': 0.9614442668416976}\n",
      "total size 18345\n",
      "total size 4876\n",
      "0.8655707304468105\n",
      "Dataset: bpic2015_1_f2\n",
      "Classifier XGB\n",
      "Encoding agg\n",
      "{'colsample_bytree': 0.7346377296325085, 'learning_rate': 0.21000157971023425, 'max_depth': 22, 'min_child_weight': 6, 'subsample': 0.5162121390745571}\n",
      "total size 18345\n",
      "total size 4876\n",
      "0.7749716352944881\n",
      "Dataset: bpic2015_2_f2\n",
      "Classifier XGB\n",
      "Encoding agg\n",
      "{'colsample_bytree': 0.5084061797669884, 'learning_rate': 0.03351554501388054, 'max_depth': 19, 'min_child_weight': 5, 'subsample': 0.7004518132765967}\n",
      "total size 22221\n",
      "total size 5789\n",
      "0.9455605736957169\n",
      "Dataset: bpic2015_2_f2\n",
      "Classifier XGB\n",
      "Encoding agg\n",
      "{'colsample_bytree': 0.9368506879192937, 'learning_rate': 0.09469487104887575, 'max_depth': 11, 'min_child_weight': 6, 'subsample': 0.5950271724196214}\n",
      "total size 22221\n",
      "total size 5789\n",
      "0.867269664557665\n",
      "Dataset: bpic2015_2_f2\n",
      "Classifier XGB\n",
      "Encoding agg\n",
      "{'colsample_bytree': 0.6357330236166518, 'learning_rate': 0.09038074861724987, 'max_depth': 20, 'min_child_weight': 1, 'subsample': 0.6584889193934408}\n",
      "total size 22221\n",
      "total size 5789\n",
      "0.8212065734130953\n",
      "Dataset: bpic2015_3_f2\n",
      "Classifier XGB\n",
      "Encoding agg\n",
      "{'colsample_bytree': 0.7138837211227853, 'learning_rate': 0.05150483016141405, 'max_depth': 30, 'min_child_weight': 4, 'subsample': 0.650088114914436}\n",
      "total size 37400\n",
      "total size 10041\n",
      "0.9479714042513507\n",
      "Dataset: bpic2015_3_f2\n",
      "Classifier XGB\n",
      "Encoding agg\n",
      "{'colsample_bytree': 0.6285707930245421, 'learning_rate': 0.0376032714824881, 'max_depth': 5, 'min_child_weight': 4, 'subsample': 0.8845094265145081}\n",
      "total size 37400\n",
      "total size 10041\n",
      "0.9346184086693197\n",
      "Dataset: bpic2015_3_f2\n",
      "Classifier XGB\n",
      "Encoding agg\n",
      "{'colsample_bytree': 0.761138228754475, 'learning_rate': 0.024946632930018753, 'max_depth': 10, 'min_child_weight': 4, 'subsample': 0.7726936797857537}\n",
      "total size 37400\n",
      "total size 10041\n",
      "0.930631297240034\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in datasets:\n",
    "    for cls_method in classifiers:\n",
    "        for cls_encoding in encoding:\n",
    "            for level in incomplete_levels: \n",
    "                print('Dataset:', dataset_name)\n",
    "                print('Classifier', cls_method)\n",
    "                print('Encoding', cls_encoding)\n",
    "                dataset_manager = DatasetManager(dataset_name)\n",
    "                dataset_name_csv = res[dataset_name]\n",
    "                data = dataset_manager.read_dataset('Original_data/'+dataset_name_csv+'.csv')\n",
    "                dataset_name_csv = res[dataset_name]\n",
    "                method_name = \"%s_%s\" % (column_selection, cls_encoding)\n",
    "                methods = encoding_dict[cls_encoding]\n",
    "\n",
    "                # extract the optimal parameters\n",
    "                optimal_params_filename = os.path.join(params_dir,\"uPU_optimal_params_%s_%s_%s_%s.pickle\" % (cls_method, dataset_name, level, method_name)\n",
    "                if not os.path.isfile(optimal_params_filename) or os.path.getsize(optimal_params_filename) <= 0:\n",
    "                    print('problem')\n",
    "                with open(optimal_params_filename, \"rb\") as fin:\n",
    "                    args = pickle.load(fin)\n",
    "                    print(args)\n",
    "\n",
    "                cls_encoder_args = {'case_id_col': dataset_manager.case_id_col,\n",
    "                                    'static_cat_cols': dataset_manager.static_cat_cols,\n",
    "                                    'static_num_cols': dataset_manager.static_num_cols,\n",
    "                                    'dynamic_cat_cols': dataset_manager.dynamic_cat_cols,\n",
    "                                    'dynamic_num_cols': dataset_manager.dynamic_num_cols,\n",
    "                                    'fillna': True}\n",
    "\n",
    "                #file to save results\n",
    "                outfile = os.path.join(results_dir, \"uPU_performance_results_%s_%s_%s_%s.csv\" % (cls_method, dataset_name, level, method_name))\n",
    "\n",
    "                # determine min and max (truncated) prefix lengths\n",
    "                min_prefix_length = 1\n",
    "                if \"traffic_fines\" in dataset_name:\n",
    "                    max_prefix_length = 10\n",
    "                elif \"bpic2017\" in dataset_name:\n",
    "                    max_prefix_length = min(20, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n",
    "                else:\n",
    "                    max_prefix_length = min(40, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n",
    "\n",
    "                maxlen = cutoff = max_prefix_length \n",
    "\n",
    "                # split into training and test\n",
    "                train = dataset_manager.read_dataset('Data/Train_PU'+level+'_'+dataset_name_csv+'.csv')\n",
    "                test = dataset_manager.read_dataset('Test_'+dataset_name_csv+'.csv')\n",
    "                #prefix generation of train and test data\n",
    "                dt_train_prefixes = dataset_manager.generate_prefix_data(train, min_prefix_length, max_prefix_length)\n",
    "                dt_test_prefixes = dataset_manager.generate_prefix_data(test, min_prefix_length, max_prefix_length)\n",
    "                test_y = dataset_manager.get_label_numeric(dt_test_prefixes)\n",
    "                train_y = dataset_manager.get_label_numeric(dt_train_prefixes)\n",
    "                dt_train_named, dt_test_named = transform_data(dt_train_prefixes, dt_test_prefixes, train_y)\n",
    "\n",
    "                #DELETE THIS LATER\n",
    "                count_labels_number(train_y)\n",
    "                count_labels_number(test_y)\n",
    "\n",
    "                #create the input layers and embeddings\n",
    "                embeddings= []\n",
    "                input_layers = []\n",
    "                preds_all = []\n",
    "                nr_events_all = []\n",
    "                nr_events = list(dataset_manager.get_prefix_lengths(dt_test_prefixes))\n",
    "                nr_events_all.extend(nr_events)\n",
    "                test_y_all = []\n",
    "                test_y_all.extend(test_y)\n",
    "                #MODEL\n",
    "\n",
    "                flip_ratio_ = levels[level]  \n",
    "                label_freq_ = 1.0 - flip_ratio_  ## P(labeled | y = 1)\n",
    "\n",
    "\n",
    "\n",
    "                current_args = args\n",
    "                cls = xgb_pu.PUBoost(obj='upu',\n",
    "                                     n_estimators=500,\n",
    "                                     learning_rate= current_args['learning_rate'],\n",
    "                                     subsample=current_args['subsample'],\n",
    "                                     max_depth=int(current_args['max_depth']),\n",
    "                                     colsample_bytree=current_args['colsample_bytree'],\n",
    "                                     min_child_weight=int(current_args['min_child_weight']),\n",
    "                                     random_state=random_state,\n",
    "                                     label_freq = label_freq_).fit(dt_train_named,train_y)\n",
    "\n",
    "                # predictions\n",
    "                pred= expit(cls.inplace_predict(dt_test_named))\n",
    "                preds_all.extend(pred)\n",
    "                auc_total = roc_auc_score(test_y_all, preds_all)\n",
    "\n",
    "                score = 0\n",
    "                dim = 0\n",
    "                auc_total = roc_auc_score(test_y_all, preds_all)\n",
    "\n",
    "                print(auc_total)\n",
    "                with open(outfile, 'w') as fout:\n",
    "                    fout.write(\"%s;%s;%s;%s;%s;%s\\n\" % (\"dataset\", \"method\", \"cls\", \"nr_events\", \"metric\", \"score\")) \n",
    "                    dt_results = pd.DataFrame({\"actual\": test_y_all, \"predicted\": preds_all, \"nr_events\": nr_events_all})\n",
    "                    for nr_events, group in dt_results.groupby(\"nr_events\"):\n",
    "                        if len(set(group.actual)) < 2:\n",
    "                            fout.write(\"%s;%s;%s;%s;%s;%s;%s\\n\" % (dataset_name, method_name, cls_method, nr_events, -1,\n",
    "                                                                   \"auc\", np.nan))\n",
    "                        else:\n",
    "                            fout.write(\"%s;%s;%s;%s;%s;%s;%s\\n\" % (dataset_name, method_name, cls_method, nr_events, -1,\n",
    "                                                                   \"auc\", roc_auc_score(group.actual, group.predicted)))\n",
    "                    fout.write(\"%s;%s;%s;%s;%s;%s\\n\" % (dataset_name, method_name, cls_method, -1, \"auc\",\n",
    "                                                        roc_auc_score(dt_results.actual, dt_results.predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T4SCL-P4E2dt",
    "outputId": "089911fa-1c96-4dc1-ced6-4fc3cae4ce20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.761138228754475,\n",
       " 'learning_rate': 0.024946632930018753,\n",
       " 'max_depth': 10,\n",
       " 'min_child_weight': 4,\n",
       " 'subsample': 0.7726936797857537}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MjW9RH8vE2du",
    "outputId": "6aef0fb2-b065-4294-8d82-0f870c715020"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'1.6.1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJM0LmP78wwG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "v0yRQ_ZSMaFG",
    "UpCBthWhMhkS",
    "HcKLuejWcyxN"
   ],
   "machine_shape": "hm",
   "name": "uPU_Experiments_XGB_(PU_OOPPM).ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
