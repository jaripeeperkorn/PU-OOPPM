{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLz0ql-Mcyw6"
   },
   "source": [
    "##############################################\n",
    "\n",
    "############# TABLE OF CONTENTS #############\n",
    "\n",
    "##############################################\n",
    "- 1) Import packages and functions\n",
    "- 2) Function for preprocessing the data\n",
    "- 3) Parameters\n",
    "\n",
    "DON'T FORGET TO ADAPT THE NUMBER OF EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHaBvAt6dAx4"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# Things to upload to your file from Google drive:\n",
    "- dataset_confs.py\n",
    "- DatasetManager.py\n",
    "- your hyperparameter file (from hyperopt, with the arguments)\n",
    "- dataset (csv file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wp-ZLQKknHr0"
   },
   "outputs": [],
   "source": [
    "#incomplete_levels = ['25', '50', '75']\n",
    "\n",
    "#levels = {'25': 0.25, '50': 0.50, '75': 0.75}\n",
    "\n",
    "incomplete_levels = ['50', '75']\n",
    "levels = {'50': 0.50, '75': 0.75}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fQuEKilNdg8s"
   },
   "outputs": [],
   "source": [
    "import pu_keras as puk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Smkva2r5rMxR"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import dataset_confs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "class DatasetManager:\n",
    "    \n",
    "    def __init__(self, dataset_name):\n",
    "        self.dataset_name = dataset_name\n",
    "        \n",
    "        self.case_id_col = dataset_confs.case_id_col[self.dataset_name]\n",
    "        self.activity_col = dataset_confs.activity_col[self.dataset_name]\n",
    "        self.timestamp_col = dataset_confs.timestamp_col[self.dataset_name]\n",
    "        self.label_col = dataset_confs.label_col[self.dataset_name]\n",
    "        self.pos_label = dataset_confs.pos_label[self.dataset_name]\n",
    "\n",
    "        self.dynamic_cat_cols = dataset_confs.dynamic_cat_cols[self.dataset_name]\n",
    "        self.static_cat_cols = dataset_confs.static_cat_cols[self.dataset_name]\n",
    "        self.dynamic_num_cols = dataset_confs.dynamic_num_cols[self.dataset_name]\n",
    "        self.static_num_cols = dataset_confs.static_num_cols[self.dataset_name]\n",
    "        \n",
    "        self.sorting_cols = [self.timestamp_col, self.activity_col]\n",
    "\n",
    "    \n",
    "    def read_dataset(self, datalocation):\n",
    "        # read dataset\n",
    "        dtypes = {col:\"object\" for col in self.dynamic_cat_cols+self.static_cat_cols+[self.case_id_col, self.label_col, self.timestamp_col]}\n",
    "        for col in self.dynamic_num_cols + self.static_num_cols:\n",
    "            dtypes[col] = \"float\"\n",
    "\n",
    "        data = pd.read_csv(datalocation, sep=\";\", dtype=dtypes)\n",
    "        data[self.timestamp_col] = pd.to_datetime(data[self.timestamp_col])\n",
    "\n",
    "        if self.dataset_name in ['bpic2011_f1', 'bpic2011_f2', 'bpic2011_f3', 'bpic2011_f4','bpic2015_1_f2','bpic2015_2_f2','bpic2015_3_f2','bpic2015_4_f2','bpic2015_5_f2','sepsis_cases_1','sepsis_cases_2','sepsis_cases_4']:\n",
    "            data['time:timestamp'] = pd.to_datetime(data['time:timestamp']) \n",
    "        if self.dataset_name in ['bpic2012_accepted', 'bpic2012_cancelled', 'bpic2012_declined']:\n",
    "            data['Complete Timestamp'] = pd.to_datetime(data['Complete Timestamp'])\n",
    "\n",
    "        return data\n",
    "    \n",
    "\n",
    "\n",
    "    def split_data(self, data, train_ratio, split=\"temporal\", seed=22):  \n",
    "        # split into train and test using temporal split\n",
    "\n",
    "        grouped = data.groupby(self.case_id_col)\n",
    "        start_timestamps = grouped[self.timestamp_col].min().reset_index()\n",
    "        if split == \"temporal\":\n",
    "            start_timestamps = start_timestamps.sort_values(self.timestamp_col, ascending=True, kind=\"mergesort\")\n",
    "        elif split == \"random\":\n",
    "            np.random.seed(seed)\n",
    "            start_timestamps = start_timestamps.reindex(np.random.permutation(start_timestamps.index))\n",
    "        train_ids = list(start_timestamps[self.case_id_col])[:int(train_ratio*len(start_timestamps))]\n",
    "        train = data[data[self.case_id_col].isin(train_ids)].sort_values(self.timestamp_col, ascending=True, kind='mergesort')\n",
    "        test = data[~data[self.case_id_col].isin(train_ids)].sort_values(self.timestamp_col, ascending=True, kind='mergesort')\n",
    "\n",
    "        return (train, test)\n",
    "    \n",
    "    def split_data_strict(self, data, train_ratio, split=\"temporal\"):  \n",
    "        # split into train and test using temporal split and discard events that overlap the periods\n",
    "        data = data.sort_values(self.sorting_cols, ascending=True, kind='mergesort')\n",
    "        grouped = data.groupby(self.case_id_col)\n",
    "        start_timestamps = grouped[self.timestamp_col].min().reset_index()\n",
    "        start_timestamps = start_timestamps.sort_values(self.timestamp_col, ascending=True, kind='mergesort')\n",
    "        train_ids = list(start_timestamps[self.case_id_col])[:int(train_ratio*len(start_timestamps))]\n",
    "        train = data[data[self.case_id_col].isin(train_ids)].sort_values(self.sorting_cols, ascending=True, kind='mergesort')\n",
    "        test = data[~data[self.case_id_col].isin(train_ids)].sort_values(self.sorting_cols, ascending=True, kind='mergesort')\n",
    "        split_ts = test[self.timestamp_col].min()\n",
    "        train = train[train[self.timestamp_col] < split_ts]\n",
    "        return (train, test)\n",
    "    \n",
    "    def split_data_discard(self, data, train_ratio, split=\"temporal\"):  \n",
    "        # split into train and test using temporal split and discard events that overlap the periods\n",
    "        data = data.sort_values(self.sorting_cols, ascending=True, kind='mergesort')\n",
    "        grouped = data.groupby(self.case_id_col)\n",
    "        start_timestamps = grouped[self.timestamp_col].min().reset_index()\n",
    "        start_timestamps = start_timestamps.sort_values(self.timestamp_col, ascending=True, kind='mergesort')\n",
    "        train_ids = list(start_timestamps[self.case_id_col])[:int(train_ratio*len(start_timestamps))]\n",
    "        train = data[data[self.case_id_col].isin(train_ids)].sort_values(self.sorting_cols, ascending=True, kind='mergesort')\n",
    "        test = data[~data[self.case_id_col].isin(train_ids)].sort_values(self.sorting_cols, ascending=True, kind='mergesort')\n",
    "        split_ts = test[self.timestamp_col].min()\n",
    "        overlapping_cases = train[train[self.timestamp_col] >= split_ts][self.case_id_col].unique()\n",
    "        train = train[~train[self.case_id_col].isin(overlapping_cases)]\n",
    "        return (train, test)\n",
    "    \n",
    "    \n",
    "    def split_val(self, data, val_ratio, split=\"random\", seed=22):  \n",
    "        # split into train and test using temporal split\n",
    "        grouped = data.groupby(self.case_id_col)\n",
    "        start_timestamps = grouped[self.timestamp_col].min().reset_index()\n",
    "        if split == \"temporal\":\n",
    "            start_timestamps = start_timestamps.sort_values(self.timestamp_col, ascending=True, kind=\"mergesort\")\n",
    "        elif split == \"random\":\n",
    "            np.random.seed(seed)\n",
    "            start_timestamps = start_timestamps.reindex(np.random.permutation(start_timestamps.index))\n",
    "        val_ids = list(start_timestamps[self.case_id_col])[-int(val_ratio*len(start_timestamps)):]\n",
    "        val = data[data[self.case_id_col].isin(val_ids)].sort_values(self.sorting_cols, ascending=True, kind=\"mergesort\")\n",
    "        train = data[~data[self.case_id_col].isin(val_ids)].sort_values(self.sorting_cols, ascending=True, kind=\"mergesort\")\n",
    "        return (train, val)\n",
    "\n",
    "\n",
    "    def generate_prefix_data(self, data, min_length, max_length, gap=1):\n",
    "        # generate prefix data (each possible prefix becomes a trace)\n",
    "        data['case_length'] = data.groupby(self.case_id_col)[self.activity_col].transform(len)\n",
    "\n",
    "        dt_prefixes = data[data['case_length'] >= min_length].groupby(self.case_id_col).head(min_length)\n",
    "        dt_prefixes[\"prefix_nr\"] = 1\n",
    "        dt_prefixes[\"orig_case_id\"] = dt_prefixes[self.case_id_col]\n",
    "        for nr_events in range(min_length+gap, max_length+1, gap):\n",
    "            tmp = data[data['case_length'] >= nr_events].groupby(self.case_id_col).head(nr_events)\n",
    "            tmp[\"orig_case_id\"] = tmp[self.case_id_col]\n",
    "            tmp[self.case_id_col] = tmp[self.case_id_col].apply(lambda x: \"%s_%s\"%(x, nr_events))\n",
    "            tmp[\"prefix_nr\"] = nr_events\n",
    "            dt_prefixes = pd.concat([dt_prefixes, tmp], axis=0)\n",
    "        \n",
    "        dt_prefixes['case_length'] = dt_prefixes['case_length'].apply(lambda x: min(max_length, x))\n",
    "        \n",
    "        return dt_prefixes\n",
    "\n",
    "\n",
    "    def get_pos_case_length_quantile(self, data, quantile=0.90):\n",
    "        return int(np.ceil(data[data[self.label_col]==self.pos_label].groupby(self.case_id_col).size().quantile(quantile)))\n",
    "\n",
    "    def get_indexes(self, data):\n",
    "        return data.groupby(self.case_id_col).first().index\n",
    "\n",
    "    def get_relevant_data_by_indexes(self, data, indexes):\n",
    "        return data[data[self.case_id_col].isin(indexes)]\n",
    "\n",
    "    def get_label(self, data):\n",
    "        return data.groupby(self.case_id_col).first()[self.label_col]\n",
    "    \n",
    "    def get_prefix_lengths(self, data):\n",
    "        return data.groupby(self.case_id_col).last()[\"prefix_nr\"]\n",
    "    \n",
    "    def get_case_ids(self, data, nr_events=1):\n",
    "        case_ids = pd.Series(data.groupby(self.case_id_col).first().index)\n",
    "        if nr_events > 1:\n",
    "            case_ids = case_ids.apply(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n",
    "        return case_ids\n",
    "    \n",
    "    def get_label_numeric(self, data):\n",
    "        y = self.get_label(data) # one row per case\n",
    "        return [1 if label == self.pos_label else 0 for label in y]\n",
    "    \n",
    "    def get_class_ratio(self, data):\n",
    "        class_freqs = data[self.label_col].value_counts()\n",
    "        return class_freqs[self.pos_label] / class_freqs.sum()\n",
    "    \n",
    "    def get_stratified_split_generator(self, data, n_splits=5, shuffle=True, random_state=22):\n",
    "        grouped_firsts = data.groupby(self.case_id_col, as_index=False).first()\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "        \n",
    "        for train_index, test_index in skf.split(grouped_firsts, grouped_firsts[self.label_col]):\n",
    "            current_train_names = grouped_firsts[self.case_id_col][train_index]\n",
    "            train_chunk = data[data[self.case_id_col].isin(current_train_names)].sort_values(self.timestamp_col, ascending=True, kind='mergesort')\n",
    "            test_chunk = data[~data[self.case_id_col].isin(current_train_names)].sort_values(self.timestamp_col, ascending=True, kind='mergesort')\n",
    "            yield (train_chunk, test_chunk)\n",
    "            \n",
    "    def get_idx_split_generator(self, dt_for_splitting, n_splits=5, shuffle=True, random_state=22):\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "        \n",
    "        for train_index, test_index in skf.split(dt_for_splitting, dt_for_splitting[self.label_col]):\n",
    "            current_train_names = dt_for_splitting[self.case_id_col][train_index]\n",
    "            current_test_names = dt_for_splitting[self.case_id_col][test_index]\n",
    "            yield (current_train_names, current_test_names)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0yRQ_ZSMaFG"
   },
   "source": [
    "# **import packages and functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uwBik9AUO-VZ"
   },
   "outputs": [],
   "source": [
    "# functions and packages\n",
    "#import EncoderFactory\n",
    "\n",
    "\n",
    "import dataset_confs\n",
    "#from DatasetManager import DatasetManager\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from pandas.api.types import is_string_dtype\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "#LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization,Masking, Dropout, Input, Multiply\n",
    "from tensorflow.keras.layers import concatenate, Embedding, LSTM, Bidirectional, TimeDistributed, Softmax, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Nadam, Adam, SGD, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import tensorflow.keras.utils as ku\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSywE9BHcyxG"
   },
   "source": [
    "## Functions with source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lUs2wzDTcyxH"
   },
   "outputs": [],
   "source": [
    "# SOURCE: https://towardsdatascience.com/using-neural-networks-with-embedding-layers-to-encode-high-cardinality-categorical-variables-c1b872033ba2\n",
    "class ColumnEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.columns = None\n",
    "        self.maps = dict()\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.columns:\n",
    "            # encode value x of col via dict entry self.maps[col][x]+1 if present, otherwise 0\n",
    "            X_copy.loc[:,col] = X_copy.loc[:,col].apply(lambda x: self.maps[col].get(x, -1)+1)\n",
    "        return X_copy\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.columns:\n",
    "            values = list(self.maps[col].keys())\n",
    "            # find value in ordered list and map out of range values to None\n",
    "            X_copy.loc[:,col] = [values[i-1] if 0<i<=len(values) else None for i in X_copy[col]]\n",
    "        return X_copy\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # only apply to string type columns\n",
    "        self.columns = [col for col in X.columns if is_string_dtype(X[col])]\n",
    "        for col in self.columns:\n",
    "            self.maps[col] = OrderedDict({value: num for num, value in enumerate(sorted(set(X[col])))})\n",
    "        return self\n",
    "\n",
    "def prepare_inputs(X_train, X_test, data):  \n",
    "    global ce\n",
    "    ce = ColumnEncoder()\n",
    "    X_train, X_test = X_train.astype(str), X_test.astype(str)\n",
    "    X_train_enc = ce.fit_transform(X_train)\n",
    "    X_test_enc = ce.transform(X_test)\n",
    "    return X_train_enc, X_test_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ud8f9rgWcyxI"
   },
   "source": [
    "## Functions from stackoverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uH3LRIBHOyap"
   },
   "outputs": [],
   "source": [
    "def numeric_padding(sequences, maxlen=None, value=0):\n",
    "    num_samples = len(sequences)\n",
    "    sample_shape = np.asarray(sequences[0]).shape[1:]\n",
    "    x = np.full((num_samples, maxlen) + sample_shape, value)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        trunc = s[:maxlen]\n",
    "        x[idx, :maxlen] = trunc[0]\n",
    "        \n",
    "def remove_punctuations(columns_before):\n",
    "    columns = []\n",
    "    for string in columns_before:\n",
    "        new_string = string.replace(\":\", \"_\")\n",
    "        columns.append(new_string)\n",
    "    return columns\n",
    "\n",
    "def create_index(log_df, column):\n",
    "    \"\"\"Creates an idx for a categorical attribute.\n",
    "    Args:\n",
    "        log_df: dataframe.\n",
    "        column: column name.\n",
    "    Returns:\n",
    "        index of a categorical attribute pairs.\n",
    "    \"\"\"\n",
    "    temp_list = temp_list = log_df[log_df[column] != 'none'][[column]].values.tolist() #remove all 'none' values from the index\n",
    "    subsec_set = {(x[0]) for x in temp_list}\n",
    "    subsec_set = sorted(list(subsec_set))\n",
    "    alias = dict()\n",
    "    if column !='next_activity':\n",
    "        for i, _ in enumerate(subsec_set):          \n",
    "            alias[subsec_set[i]] = i + 1\n",
    "        alias['none'] = 0\n",
    "    else:\n",
    "        for i, _ in enumerate(subsec_set):\n",
    "            alias[subsec_set[i]] = i  \n",
    "    #reorder by the index value\n",
    "    alias = {k: v for k, v in sorted(alias.items(), key=lambda item: item[1])}\n",
    "    return alias\n",
    "\n",
    "#call this function with the name of the right column\n",
    "def create_indexes(i, data):\n",
    "    cat_index = create_index(data, i)\n",
    "    cat_index['Start'] = 0\n",
    "    cat_index['End'] = len(cat_index)\n",
    "    index_cat = {v: k for k, v in cat_index.items()}\n",
    "    cat_weights = ku.to_categorical(sorted(index_cat.keys()), len(cat_index))\n",
    "    return cat_weights, index_cat, cat_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7qIHUALcyxK"
   },
   "source": [
    "## From from Alexander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_5KAPqhzcyxL"
   },
   "outputs": [],
   "source": [
    "def groupby_caseID(data, cols):\n",
    "    ans = [pd.DataFrame(y) for x, y in data[cols].groupby('Case ID', as_index=False)]\n",
    "    return ans\n",
    "def pad_cat_data(cols, data_train, data_test, maxlen):\n",
    "    \n",
    "    #padding of the different categorical columns\n",
    "    #train paddings\n",
    "    paddings_train = []\n",
    "    for i in cols:\n",
    "        padding= []\n",
    "        for k in range(0,len(data_train)):\n",
    "            temp = []\n",
    "            temp = list(data_train[k][i])\n",
    "            padding.append(temp)\n",
    "        padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n",
    "        #padded = padded/len(data.groupby([i]))\n",
    "        paddings_train.append(padded)\n",
    "\n",
    "    #test paddings\n",
    "    paddings_test = []\n",
    "    for i in cols:\n",
    "        padding= []\n",
    "        for k in range(0,len(data_test)):\n",
    "            temp = []\n",
    "            temp = list(data_test[k][i])\n",
    "            padding.append(temp)\n",
    "        padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n",
    "        #padded = padded/len(data.groupby([i]))\n",
    "        paddings_test.append(padded)\n",
    "    return paddings_train, paddings_test\n",
    "\n",
    "def pad_num_data(cols, data_train, data_test, maxlen, dt_train_prefixes, dt_test_prefixes):\n",
    "    pad_train = []\n",
    "    pad_test  = []\n",
    "    for i in cols:\n",
    "        \n",
    "        padding = []\n",
    "        for k in range(0,len(data_train)):\n",
    "            temp_train = []\n",
    "            temp_train = list(data_train[k][i])\n",
    "            padding.append(temp_train)\n",
    "\n",
    "        padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n",
    "        if dt_train_prefixes[i].max() !=0:\n",
    "           \n",
    "            padded = padded/dt_train_prefixes[i].max()\n",
    "        else:\n",
    "            padded = padded\n",
    "        pad_train.append(padded)\n",
    "   \n",
    "    for i in cols:\n",
    "      \n",
    "        padding = []\n",
    "        for k in range(0,len(data_test)):\n",
    "            temp_test = []\n",
    "            temp_test = list(data_test[k][i])\n",
    "            padding.append(temp_test)\n",
    "      \n",
    "        padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n",
    "        if dt_test_prefixes[i].max() !=0:\n",
    "            padded = padded/dt_test_prefixes[i].max()\n",
    "        else:\n",
    "            padded = padded\n",
    "        pad_test.append(padded)\n",
    "    return pad_train, pad_test\n",
    "\n",
    "def reshape_num_data(pad_data, cutoff):\n",
    "        pad_num = np.reshape(pad_data, (len(pad_data), cutoff, 1))\n",
    "        return pad_num\n",
    "def labels_after_grouping(data_train,data_test):\n",
    "    train_labels = []\n",
    "    for i in range (0,len(data_train)):\n",
    "        temp_label = data_train[i]['label'].iloc[0]\n",
    "        train_labels.append(temp_label)\n",
    "\n",
    "    test_labels = []\n",
    "    for i in range (0,len(data_test)):\n",
    "        temp_label = data_test[i]['label'].iloc[0]\n",
    "        test_labels.append(temp_label)\n",
    "    train_y = [1 if i!='regular' else 0 for i in train_labels]\n",
    "    test_y = [1 if i!='regular' else 0 for i in test_labels]\n",
    "    return train_y, test_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpCBthWhMhkS"
   },
   "source": [
    "# **Function for preprocessing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "l7tBM8hYcyxM"
   },
   "outputs": [],
   "source": [
    "def create_data(dt_train_prefixes, dt_test_prefixes, cat_cols, numerical_cols):\n",
    "    #get the label of the train and test set\n",
    "    test_y = dataset_manager.get_label_numeric(dt_test_prefixes)\n",
    "    train_y = dataset_manager.get_label_numeric(dt_train_prefixes)   \n",
    "\n",
    "    dt_train_prefixes[cat_cols],dt_test_prefixes[cat_cols]= prepare_inputs(dt_train_prefixes[cat_cols], dt_test_prefixes[cat_cols], data)\n",
    "    dt_train_prefixes[cat_cols] = dt_train_prefixes[cat_cols]+1\n",
    "    dt_test_prefixes[cat_cols] = dt_test_prefixes[cat_cols]+1\n",
    "    \n",
    "    #append caseId and label\n",
    "    cat_cols.append('Case ID')\n",
    "    cat_cols.append('label')\n",
    "    \n",
    "    #groupby case ID\n",
    "    ans_train = groupby_caseID(dt_train_prefixes, cat_cols)\n",
    "    ans_test = groupby_caseID(dt_test_prefixes, cat_cols)\n",
    "    #obtain the new label lists after grouping\n",
    "    train_y, test_y = labels_after_grouping(ans_train, ans_test)\n",
    "    #remove then back\n",
    "    cat_cols.remove('label')\n",
    "    cat_cols.remove('Case ID')\n",
    "    #pad cat columns\n",
    "    paddings_train, paddings_test = pad_cat_data(cat_cols, ans_train, ans_test, maxlen)\n",
    "  \n",
    "    #NUMERICAL COLUMNS\n",
    "\n",
    "    numerical_columns.append('Case ID')\n",
    "    ans_train2 = groupby_caseID(dt_train_prefixes, numerical_columns)\n",
    "    ans_test2 = groupby_caseID(dt_test_prefixes, numerical_columns )\n",
    "    numerical_columns.remove('Case ID')  \n",
    "    pad_train, pad_test = pad_num_data(numerical_columns, ans_train2, ans_test2, maxlen, dt_train_prefixes, dt_test_prefixes)\n",
    "\n",
    "    return pad_train, pad_test, paddings_train, paddings_test, train_y, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkzXHGIWjOVO"
   },
   "source": [
    "# Function to flip labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "u8xAQSa9jNub"
   },
   "outputs": [],
   "source": [
    "def count_labels(data_y):\n",
    "    print(\"total size\", len(data_y))\n",
    "    print(\"regular\", data_y.count(\"regular\"))\n",
    "    print(\"deviant\", data_y.count(\"deviant\"))\n",
    "\n",
    "def count_labels_number(data_y):\n",
    "    print(\"total size\", len(data_y))\n",
    "    print(\"regular\", data_y.count(0))\n",
    "    print(\"deviant\", data_y.count(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcKLuejWcyxN"
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ufibRgR8cyxO"
   },
   "outputs": [],
   "source": [
    "######PARAMETERS\n",
    "params_dir = 'params'\n",
    "results_dir ='results'\n",
    "column_selection = 'all'\n",
    "train_ratio = 0.8\n",
    "n_splits = 3\n",
    "random_state = 22\n",
    "n_iter=1\n",
    "\n",
    "encoding = ['embeddings']\n",
    "cls_method ='LSTM'\n",
    "\n",
    "csv_files = {\n",
    "    #\"bpic2011\": [\"BPIC11_f%s\"%formula for formula in range(3,4)],\n",
    "    \"bpic2015\": [\"BPIC15_%s_f2\"%(municipality) for municipality in range(3,4)],\n",
    "    #\"sepsis_cases\": [\"sepsis_cases_4\"],\n",
    "    #\"bpic2012\": [\"bpic2012_O_ACCEPTED#COMPLETE\",\"bpic2012_O_CANCELLED-COMPLETE\",\"bpic2012_0_DECLINED-COMPLETE\"],\n",
    "    #production\": [\"Production\"],\n",
    "    #\"bpic2017\": [\"BPIC17_O_Accepted\",\"BPIC17_O_Cancelled\",\"BPIC17_0_Refused\"],\n",
    "    #\"bpic2017\": [\"BPIC17_O_Cancelled\"],\n",
    "    #\"traffic_fines\": [\"traffic_fines_%s\"%formula for formula in range(1,3)],\n",
    "    #\"hospital_billing\": [\"hospital_billing_%s\"%suffix for suffix in [2,3]]\n",
    "}\n",
    "files = []\n",
    "for k, v in csv_files.items():\n",
    "    files.extend(v)\n",
    "dataset_ref_to_datasets = {\n",
    "    #\"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(3,4)],\n",
    "    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(3,4)],\n",
    "    #\"sepsis_cases\": [\"sepsis_cases_4\"]\n",
    "    #\"bpic2012\": [\"bpic2012_accepted\",\"bpic2012_cancelled\",\"bpic2012_declined\"],\n",
    "    #\"production\": [\"production\"],\n",
    "    #\"bpic2017\": [\"bpic2017_cancelled\"],\n",
    "    #\"bpic2017\": [\"bpic2017_accepted\",\"bpic2017_cancelled\",\"bpic2017_refused\"],\n",
    "    #\"traffic_fines\": [\"traffic_fines_%s\"%formula for formula in range(1,3)],\n",
    "    #\"hospital_billing\": [\"hospital_billing_%s\"%suffix for suffix in [2,3]]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "files = []\n",
    "for k, v in csv_files.items():\n",
    "    files.extend(v)\n",
    "datasets = []\n",
    "for k, v in dataset_ref_to_datasets.items():\n",
    "    datasets.extend(v)\n",
    "res = {datasets[i]: files[i] for i in range(len(datasets))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVX1WhP2Mvv-"
   },
   "source": [
    "# **loop over datasets and classifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "agyg5xxq09yo",
    "outputId": "064184ac-ac1d-460f-c98c-e93bef7191ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: bpic2015_3_f2\n",
      "Classifier LSTM\n",
      "Encoding embeddings\n",
      "{'LSTM_dropout': 0.13974149171990055, 'batch_size': 192, 'learning_rate': 0.007966382215671723, 'lstm_size': 128, 'optimizer': 'RMSprop'}\n",
      "total size 37400\n",
      "regular 33847\n",
      "deviant 3553\n",
      "total size 10041\n",
      "regular 7468\n",
      "deviant 2573\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Activity (InputLayer)          [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " monitoringResource (InputLayer  [(None, 40)]        0           []                               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " question (InputLayer)          [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " org_resource (InputLayer)      [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " Responsible_actor (InputLayer)  [(None, 40)]        0           []                               \n",
      "                                                                                                  \n",
      " embed_Activity (Embedding)     (None, 40, 383)      146306      ['Activity[0][0]']               \n",
      "                                                                                                  \n",
      " embed_monitoringResource (Embe  (None, 40, 21)      420         ['monitoringResource[0][0]']     \n",
      " dding)                                                                                           \n",
      "                                                                                                  \n",
      " embed_question (Embedding)     (None, 40, 19)       342         ['question[0][0]']               \n",
      "                                                                                                  \n",
      " embed_org_resource (Embedding)  (None, 40, 17)      272         ['org_resource[0][0]']           \n",
      "                                                                                                  \n",
      " embed_Responsible_actor (Embed  (None, 40, 21)      420         ['Responsible_actor[0][0]']      \n",
      " ding)                                                                                            \n",
      "                                                                                                  \n",
      " hour (InputLayer)              [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " weekday (InputLayer)           [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " month (InputLayer)             [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " timesincemidnight (InputLayer)  [(None, 40, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " timesincelastevent (InputLayer  [(None, 40, 1)]     0           []                               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " timesincecasestart (InputLayer  [(None, 40, 1)]     0           []                               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " event_nr (InputLayer)          [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " open_cases (InputLayer)        [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " SUMleges (InputLayer)          [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " Aanleg__Uitvoeren_werk_of_werk  [(None, 40, 1)]     0           []                               \n",
      " zaamheid_ (InputLayer)                                                                           \n",
      "                                                                                                  \n",
      " Bouw (InputLayer)              [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " Brandveilig_gebruik__vergunnin  [(None, 40, 1)]     0           []                               \n",
      " g_ (InputLayer)                                                                                  \n",
      "                                                                                                  \n",
      " Gebiedsbescherming (InputLayer  [(None, 40, 1)]     0           []                               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " Handelen_in_strijd_met_regels_  [(None, 40, 1)]     0           []                               \n",
      " RO (InputLayer)                                                                                  \n",
      "                                                                                                  \n",
      " Inrit/Uitweg (InputLayer)      [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " Kap (InputLayer)               [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " Milieu__neutraal_wijziging_ (I  [(None, 40, 1)]     0           []                               \n",
      " nputLayer)                                                                                       \n",
      "                                                                                                  \n",
      " Milieu__omgevingsvergunning_be  [(None, 40, 1)]     0           []                               \n",
      " perkte_milieutoets_ (InputLaye                                                                   \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " Milieu__vergunning_ (InputLaye  [(None, 40, 1)]     0           []                               \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " Monument (InputLayer)          [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " Reclame (InputLayer)           [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " Sloop (InputLayer)             [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " Flora_en_Fauna (InputLayer)    [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " Brandveilig_gebruik__melding_   [(None, 40, 1)]     0           []                               \n",
      " (InputLayer)                                                                                     \n",
      "                                                                                                  \n",
      " Milieu__melding_ (InputLayer)  [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " full_embedding (Concatenate)   (None, 40, 486)      0           ['embed_Activity[0][0]',         \n",
      "                                                                  'embed_monitoringResource[0][0]'\n",
      "                                                                 , 'embed_question[0][0]',        \n",
      "                                                                  'embed_org_resource[0][0]',     \n",
      "                                                                  'embed_Responsible_actor[0][0]',\n",
      "                                                                  'hour[0][0]',                   \n",
      "                                                                  'weekday[0][0]',                \n",
      "                                                                  'month[0][0]',                  \n",
      "                                                                  'timesincemidnight[0][0]',      \n",
      "                                                                  'timesincelastevent[0][0]',     \n",
      "                                                                  'timesincecasestart[0][0]',     \n",
      "                                                                  'event_nr[0][0]',               \n",
      "                                                                  'open_cases[0][0]',             \n",
      "                                                                  'SUMleges[0][0]',               \n",
      "                                                                  'Aanleg__Uitvoeren_werk_of_werkz\n",
      "                                                                 aamheid_[0][0]',                 \n",
      "                                                                  'Bouw[0][0]',                   \n",
      "                                                                  'Brandveilig_gebruik__vergunning\n",
      "                                                                 _[0][0]',                        \n",
      "                                                                  'Gebiedsbescherming[0][0]',     \n",
      "                                                                  'Handelen_in_strijd_met_regels_R\n",
      "                                                                 O[0][0]',                        \n",
      "                                                                  'Inrit/Uitweg[0][0]',           \n",
      "                                                                  'Kap[0][0]',                    \n",
      "                                                                  'Milieu__neutraal_wijziging_[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'Milieu__omgevingsvergunning_bep\n",
      "                                                                 erkte_milieutoets_[0][0]',       \n",
      "                                                                  'Milieu__vergunning_[0][0]',    \n",
      "                                                                  'Monument[0][0]',               \n",
      "                                                                  'Reclame[0][0]',                \n",
      "                                                                  'Sloop[0][0]',                  \n",
      "                                                                  'Flora_en_Fauna[0][0]',         \n",
      "                                                                  'Brandveilig_gebruik__melding_[0\n",
      "                                                                 ][0]',                           \n",
      "                                                                  'Milieu__melding_[0][0]']       \n",
      "                                                                                                  \n",
      " LSTM1 (Bidirectional)          (None, 40, 256)      629760      ['full_embedding[0][0]']         \n",
      "                                                                                                  \n",
      " LSTM2 (Bidirectional)          (None, 256)          394240      ['LSTM1[0][0]']                  \n",
      "                                                                                                  \n",
      " final_output (Dense)           (None, 1)            257         ['LSTM2[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,172,017\n",
      "Trainable params: 1,172,017\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\u0132580\\AppData\\Local\\Temp\\__autograph_generated_filerplxedc6.py:30: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  ag__.if_stmt((ag__.ld(label_smoothing) is not 0), if_body, else_body, get_state, set_state, ('y_true',), 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 - 20s - loss: 0.3531 - val_loss: 0.3120 - lr: 0.0080 - 20s/epoch - 113ms/step\n",
      "Epoch 2/300\n",
      "176/176 - 12s - loss: 0.2531 - val_loss: 0.3244 - lr: 0.0080 - 12s/epoch - 69ms/step\n",
      "Epoch 3/300\n",
      "176/176 - 12s - loss: 0.1693 - val_loss: 0.5554 - lr: 0.0080 - 12s/epoch - 68ms/step\n",
      "Epoch 4/300\n",
      "176/176 - 12s - loss: 0.1193 - val_loss: 0.5405 - lr: 0.0080 - 12s/epoch - 70ms/step\n",
      "Epoch 5/300\n",
      "176/176 - 12s - loss: 0.1016 - val_loss: 0.8965 - lr: 0.0080 - 12s/epoch - 67ms/step\n",
      "Epoch 6/300\n",
      "176/176 - 12s - loss: 0.0895 - val_loss: 0.6363 - lr: 0.0080 - 12s/epoch - 68ms/step\n",
      "Epoch 7/300\n",
      "176/176 - 12s - loss: 0.0768 - val_loss: 0.9589 - lr: 0.0080 - 12s/epoch - 68ms/step\n",
      "Epoch 8/300\n",
      "176/176 - 12s - loss: 0.0737 - val_loss: 0.7712 - lr: 0.0080 - 12s/epoch - 68ms/step\n",
      "Epoch 9/300\n",
      "176/176 - 12s - loss: 0.0711 - val_loss: 0.7634 - lr: 0.0080 - 12s/epoch - 68ms/step\n",
      "Epoch 10/300\n",
      "176/176 - 12s - loss: 0.0678 - val_loss: 0.9371 - lr: 0.0080 - 12s/epoch - 68ms/step\n",
      "Epoch 11/300\n",
      "176/176 - 13s - loss: 0.0621 - val_loss: 1.0355 - lr: 0.0080 - 13s/epoch - 73ms/step\n",
      "Epoch 12/300\n",
      "176/176 - 13s - loss: 0.0502 - val_loss: 1.3168 - lr: 0.0040 - 13s/epoch - 72ms/step\n",
      "Epoch 13/300\n",
      "176/176 - 12s - loss: 0.0504 - val_loss: 1.1873 - lr: 0.0040 - 12s/epoch - 69ms/step\n",
      "Epoch 14/300\n",
      "176/176 - 12s - loss: 0.0501 - val_loss: 1.2452 - lr: 0.0040 - 12s/epoch - 70ms/step\n",
      "Epoch 15/300\n",
      "176/176 - 14s - loss: 0.0479 - val_loss: 1.2254 - lr: 0.0040 - 14s/epoch - 77ms/step\n",
      "Epoch 16/300\n",
      "176/176 - 12s - loss: 0.0496 - val_loss: 1.4010 - lr: 0.0040 - 12s/epoch - 70ms/step\n",
      "Epoch 17/300\n",
      "176/176 - 12s - loss: 0.0471 - val_loss: 1.5750 - lr: 0.0040 - 12s/epoch - 70ms/step\n",
      "Epoch 18/300\n",
      "176/176 - 13s - loss: 0.0459 - val_loss: 1.3248 - lr: 0.0040 - 13s/epoch - 74ms/step\n",
      "Epoch 19/300\n",
      "176/176 - 12s - loss: 0.0453 - val_loss: 1.3921 - lr: 0.0040 - 12s/epoch - 70ms/step\n",
      "Epoch 20/300\n",
      "176/176 - 12s - loss: 0.0467 - val_loss: 1.2334 - lr: 0.0040 - 12s/epoch - 70ms/step\n",
      "Epoch 21/300\n",
      "176/176 - 12s - loss: 0.0461 - val_loss: 1.3519 - lr: 0.0040 - 12s/epoch - 70ms/step\n",
      "Epoch 22/300\n",
      "176/176 - 12s - loss: 0.0398 - val_loss: 1.5691 - lr: 0.0020 - 12s/epoch - 69ms/step\n",
      "Epoch 23/300\n",
      "176/176 - 12s - loss: 0.0394 - val_loss: 1.6818 - lr: 0.0020 - 12s/epoch - 69ms/step\n",
      "Epoch 24/300\n",
      "176/176 - 12s - loss: 0.0393 - val_loss: 1.4608 - lr: 0.0020 - 12s/epoch - 70ms/step\n",
      "Epoch 25/300\n",
      "176/176 - 12s - loss: 0.0408 - val_loss: 1.5440 - lr: 0.0020 - 12s/epoch - 69ms/step\n",
      "Epoch 26/300\n",
      "176/176 - 12s - loss: 0.0391 - val_loss: 1.3401 - lr: 0.0020 - 12s/epoch - 69ms/step\n",
      "Epoch 27/300\n",
      "176/176 - 12s - loss: 0.0395 - val_loss: 1.3203 - lr: 0.0020 - 12s/epoch - 70ms/step\n",
      "Epoch 28/300\n",
      "176/176 - 12s - loss: 0.0375 - val_loss: 1.3736 - lr: 0.0020 - 12s/epoch - 70ms/step\n",
      "Epoch 29/300\n",
      "176/176 - 12s - loss: 0.0387 - val_loss: 1.4121 - lr: 0.0020 - 12s/epoch - 71ms/step\n",
      "Epoch 30/300\n",
      "176/176 - 12s - loss: 0.0385 - val_loss: 1.7787 - lr: 0.0020 - 12s/epoch - 69ms/step\n",
      "Epoch 31/300\n",
      "176/176 - 12s - loss: 0.0388 - val_loss: 1.5335 - lr: 0.0020 - 12s/epoch - 70ms/step\n",
      "Epoch 32/300\n",
      "176/176 - 12s - loss: 0.0368 - val_loss: 1.6467 - lr: 9.9580e-04 - 12s/epoch - 71ms/step\n",
      "Epoch 33/300\n",
      "176/176 - 13s - loss: 0.0363 - val_loss: 1.7450 - lr: 9.9580e-04 - 13s/epoch - 71ms/step\n",
      "Epoch 34/300\n",
      "176/176 - 12s - loss: 0.0346 - val_loss: 1.7233 - lr: 9.9580e-04 - 12s/epoch - 70ms/step\n",
      "Epoch 35/300\n",
      "176/176 - 12s - loss: 0.0345 - val_loss: 1.6227 - lr: 9.9580e-04 - 12s/epoch - 69ms/step\n",
      "Epoch 36/300\n",
      "176/176 - 12s - loss: 0.0345 - val_loss: 1.9047 - lr: 9.9580e-04 - 12s/epoch - 68ms/step\n",
      "Epoch 37/300\n",
      "176/176 - 13s - loss: 0.0330 - val_loss: 2.0393 - lr: 9.9580e-04 - 13s/epoch - 72ms/step\n",
      "Epoch 38/300\n",
      "176/176 - 13s - loss: 0.0350 - val_loss: 1.6938 - lr: 9.9580e-04 - 13s/epoch - 72ms/step\n",
      "Epoch 39/300\n",
      "176/176 - 12s - loss: 0.0347 - val_loss: 1.5880 - lr: 9.9580e-04 - 12s/epoch - 71ms/step\n",
      "Epoch 40/300\n",
      "176/176 - 12s - loss: 0.0330 - val_loss: 1.5974 - lr: 9.9580e-04 - 12s/epoch - 69ms/step\n",
      "Epoch 41/300\n",
      "176/176 - 12s - loss: 0.0343 - val_loss: 1.5427 - lr: 9.9580e-04 - 12s/epoch - 69ms/step\n",
      "Epoch 42/300\n",
      "176/176 - 12s - loss: 0.0314 - val_loss: 1.6161 - lr: 4.9790e-04 - 12s/epoch - 71ms/step\n",
      "Epoch 43/300\n",
      "176/176 - 12s - loss: 0.0307 - val_loss: 1.6459 - lr: 4.9790e-04 - 12s/epoch - 70ms/step\n",
      "Dataset: bpic2015_3_f2\n",
      "Classifier LSTM\n",
      "Encoding embeddings\n",
      "{'LSTM_dropout': 0.034891398452438355, 'batch_size': 176, 'learning_rate': 0.007093831190171817, 'lstm_size': 256, 'optimizer': 'Nadam'}\n",
      "total size 37400\n",
      "regular 35571\n",
      "deviant 1829\n",
      "total size 10041\n",
      "regular 7468\n",
      "deviant 2573\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Activity (InputLayer)          [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " monitoringResource (InputLayer  [(None, 40)]        0           []                               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " question (InputLayer)          [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " org_resource (InputLayer)      [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " Responsible_actor (InputLayer)  [(None, 40)]        0           []                               \n",
      "                                                                                                  \n",
      " embed_Activity (Embedding)     (None, 40, 383)      146306      ['Activity[0][0]']               \n",
      "                                                                                                  \n",
      " embed_monitoringResource (Embe  (None, 40, 21)      420         ['monitoringResource[0][0]']     \n",
      " dding)                                                                                           \n",
      "                                                                                                  \n",
      " embed_question (Embedding)     (None, 40, 19)       342         ['question[0][0]']               \n",
      "                                                                                                  \n",
      " embed_org_resource (Embedding)  (None, 40, 17)      272         ['org_resource[0][0]']           \n",
      "                                                                                                  \n",
      " embed_Responsible_actor (Embed  (None, 40, 21)      420         ['Responsible_actor[0][0]']      \n",
      " ding)                                                                                            \n",
      "                                                                                                  \n",
      " hour (InputLayer)              [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " weekday (InputLayer)           [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " month (InputLayer)             [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " timesincemidnight (InputLayer)  [(None, 40, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " timesincelastevent (InputLayer  [(None, 40, 1)]     0           []                               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " timesincecasestart (InputLayer  [(None, 40, 1)]     0           []                               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " event_nr (InputLayer)          [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " open_cases (InputLayer)        [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " SUMleges (InputLayer)          [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " Aanleg__Uitvoeren_werk_of_werk  [(None, 40, 1)]     0           []                               \n",
      " zaamheid_ (InputLayer)                                                                           \n",
      "                                                                                                  \n",
      " Bouw (InputLayer)              [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " Brandveilig_gebruik__vergunnin  [(None, 40, 1)]     0           []                               \n",
      " g_ (InputLayer)                                                                                  \n",
      "                                                                                                  \n",
      " Gebiedsbescherming (InputLayer  [(None, 40, 1)]     0           []                               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " Handelen_in_strijd_met_regels_  [(None, 40, 1)]     0           []                               \n",
      " RO (InputLayer)                                                                                  \n",
      "                                                                                                  \n",
      " Inrit/Uitweg (InputLayer)      [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " Kap (InputLayer)               [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " Milieu__neutraal_wijziging_ (I  [(None, 40, 1)]     0           []                               \n",
      " nputLayer)                                                                                       \n",
      "                                                                                                  \n",
      " Milieu__omgevingsvergunning_be  [(None, 40, 1)]     0           []                               \n",
      " perkte_milieutoets_ (InputLaye                                                                   \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " Milieu__vergunning_ (InputLaye  [(None, 40, 1)]     0           []                               \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " Monument (InputLayer)          [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " Reclame (InputLayer)           [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " Sloop (InputLayer)             [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " Flora_en_Fauna (InputLayer)    [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " Brandveilig_gebruik__melding_   [(None, 40, 1)]     0           []                               \n",
      " (InputLayer)                                                                                     \n",
      "                                                                                                  \n",
      " Milieu__melding_ (InputLayer)  [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " full_embedding (Concatenate)   (None, 40, 486)      0           ['embed_Activity[0][0]',         \n",
      "                                                                  'embed_monitoringResource[0][0]'\n",
      "                                                                 , 'embed_question[0][0]',        \n",
      "                                                                  'embed_org_resource[0][0]',     \n",
      "                                                                  'embed_Responsible_actor[0][0]',\n",
      "                                                                  'hour[0][0]',                   \n",
      "                                                                  'weekday[0][0]',                \n",
      "                                                                  'month[0][0]',                  \n",
      "                                                                  'timesincemidnight[0][0]',      \n",
      "                                                                  'timesincelastevent[0][0]',     \n",
      "                                                                  'timesincecasestart[0][0]',     \n",
      "                                                                  'event_nr[0][0]',               \n",
      "                                                                  'open_cases[0][0]',             \n",
      "                                                                  'SUMleges[0][0]',               \n",
      "                                                                  'Aanleg__Uitvoeren_werk_of_werkz\n",
      "                                                                 aamheid_[0][0]',                 \n",
      "                                                                  'Bouw[0][0]',                   \n",
      "                                                                  'Brandveilig_gebruik__vergunning\n",
      "                                                                 _[0][0]',                        \n",
      "                                                                  'Gebiedsbescherming[0][0]',     \n",
      "                                                                  'Handelen_in_strijd_met_regels_R\n",
      "                                                                 O[0][0]',                        \n",
      "                                                                  'Inrit/Uitweg[0][0]',           \n",
      "                                                                  'Kap[0][0]',                    \n",
      "                                                                  'Milieu__neutraal_wijziging_[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'Milieu__omgevingsvergunning_bep\n",
      "                                                                 erkte_milieutoets_[0][0]',       \n",
      "                                                                  'Milieu__vergunning_[0][0]',    \n",
      "                                                                  'Monument[0][0]',               \n",
      "                                                                  'Reclame[0][0]',                \n",
      "                                                                  'Sloop[0][0]',                  \n",
      "                                                                  'Flora_en_Fauna[0][0]',         \n",
      "                                                                  'Brandveilig_gebruik__melding_[0\n",
      "                                                                 ][0]',                           \n",
      "                                                                  'Milieu__melding_[0][0]']       \n",
      "                                                                                                  \n",
      " LSTM1 (Bidirectional)          (None, 40, 512)      1521664     ['full_embedding[0][0]']         \n",
      "                                                                                                  \n",
      " LSTM2 (Bidirectional)          (None, 512)          1574912     ['LSTM1[0][0]']                  \n",
      "                                                                                                  \n",
      " final_output (Dense)           (None, 1)            513         ['LSTM2[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,244,849\n",
      "Trainable params: 3,244,849\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "192/192 - 32s - loss: 0.3569 - val_loss: 0.4345 - lr: 0.0071 - 32s/epoch - 168ms/step\n",
      "Epoch 2/300\n",
      "192/192 - 26s - loss: 0.2021 - val_loss: 0.3942 - lr: 0.0071 - 26s/epoch - 137ms/step\n",
      "Epoch 3/300\n",
      "192/192 - 27s - loss: 0.1198 - val_loss: 0.3177 - lr: 0.0071 - 27s/epoch - 140ms/step\n",
      "Epoch 4/300\n",
      "192/192 - 27s - loss: 0.0870 - val_loss: 0.4148 - lr: 0.0071 - 27s/epoch - 140ms/step\n",
      "Epoch 5/300\n",
      "192/192 - 26s - loss: 0.0736 - val_loss: 0.2719 - lr: 0.0071 - 26s/epoch - 137ms/step\n",
      "Epoch 6/300\n",
      "192/192 - 28s - loss: 0.0667 - val_loss: 0.4551 - lr: 0.0071 - 28s/epoch - 147ms/step\n",
      "Epoch 7/300\n",
      "192/192 - 26s - loss: 0.0591 - val_loss: 0.5257 - lr: 0.0071 - 26s/epoch - 138ms/step\n",
      "Epoch 8/300\n",
      "192/192 - 26s - loss: 0.0561 - val_loss: 0.4372 - lr: 0.0071 - 26s/epoch - 137ms/step\n",
      "Epoch 9/300\n",
      "192/192 - 26s - loss: 0.0493 - val_loss: 0.4717 - lr: 0.0071 - 26s/epoch - 137ms/step\n",
      "Epoch 10/300\n",
      "192/192 - 27s - loss: 0.0485 - val_loss: 0.7253 - lr: 0.0071 - 27s/epoch - 139ms/step\n",
      "Epoch 11/300\n",
      "192/192 - 26s - loss: 0.0578 - val_loss: 0.5174 - lr: 0.0071 - 26s/epoch - 137ms/step\n",
      "Epoch 12/300\n",
      "192/192 - 27s - loss: 0.0534 - val_loss: 0.5841 - lr: 0.0071 - 27s/epoch - 142ms/step\n",
      "Epoch 13/300\n",
      "192/192 - 27s - loss: 0.0472 - val_loss: 0.5645 - lr: 0.0071 - 27s/epoch - 141ms/step\n",
      "Epoch 14/300\n",
      "192/192 - 26s - loss: 0.0560 - val_loss: 0.5111 - lr: 0.0071 - 26s/epoch - 136ms/step\n",
      "Epoch 15/300\n",
      "192/192 - 26s - loss: 0.0523 - val_loss: 0.9893 - lr: 0.0071 - 26s/epoch - 136ms/step\n",
      "Epoch 16/300\n",
      "192/192 - 27s - loss: 0.0386 - val_loss: 1.0318 - lr: 0.0035 - 27s/epoch - 140ms/step\n",
      "Epoch 17/300\n",
      "192/192 - 27s - loss: 0.0333 - val_loss: 1.1237 - lr: 0.0035 - 27s/epoch - 139ms/step\n",
      "Epoch 18/300\n",
      "192/192 - 27s - loss: 0.0314 - val_loss: 1.2734 - lr: 0.0035 - 27s/epoch - 139ms/step\n",
      "Epoch 19/300\n",
      "192/192 - 27s - loss: 0.0294 - val_loss: 1.3657 - lr: 0.0035 - 27s/epoch - 140ms/step\n",
      "Epoch 20/300\n",
      "192/192 - 26s - loss: 0.0293 - val_loss: 1.3409 - lr: 0.0035 - 26s/epoch - 138ms/step\n",
      "Epoch 21/300\n",
      "192/192 - 27s - loss: 0.0335 - val_loss: 1.2195 - lr: 0.0035 - 27s/epoch - 139ms/step\n",
      "Epoch 22/300\n",
      "192/192 - 27s - loss: 0.0323 - val_loss: 1.2472 - lr: 0.0035 - 27s/epoch - 138ms/step\n",
      "Epoch 23/300\n",
      "192/192 - 26s - loss: 0.0308 - val_loss: 1.3114 - lr: 0.0035 - 26s/epoch - 135ms/step\n",
      "Epoch 24/300\n",
      "192/192 - 26s - loss: 0.0300 - val_loss: 1.3930 - lr: 0.0035 - 26s/epoch - 135ms/step\n",
      "Epoch 25/300\n",
      "192/192 - 26s - loss: 0.0285 - val_loss: 1.3303 - lr: 0.0035 - 26s/epoch - 134ms/step\n",
      "Epoch 26/300\n",
      "192/192 - 26s - loss: 0.0263 - val_loss: 1.4298 - lr: 0.0018 - 26s/epoch - 134ms/step\n",
      "Epoch 27/300\n",
      "192/192 - 26s - loss: 0.0253 - val_loss: 1.4518 - lr: 0.0018 - 26s/epoch - 134ms/step\n",
      "Epoch 28/300\n",
      "192/192 - 26s - loss: 0.0241 - val_loss: 1.4607 - lr: 0.0018 - 26s/epoch - 134ms/step\n",
      "Epoch 29/300\n",
      "192/192 - 26s - loss: 0.0246 - val_loss: 1.4876 - lr: 0.0018 - 26s/epoch - 134ms/step\n",
      "Epoch 30/300\n",
      "192/192 - 26s - loss: 0.0236 - val_loss: 1.5823 - lr: 0.0018 - 26s/epoch - 134ms/step\n",
      "Epoch 31/300\n",
      "192/192 - 26s - loss: 0.0228 - val_loss: 1.5334 - lr: 0.0018 - 26s/epoch - 134ms/step\n",
      "Epoch 32/300\n",
      "192/192 - 26s - loss: 0.0231 - val_loss: 1.5462 - lr: 0.0018 - 26s/epoch - 136ms/step\n",
      "Epoch 33/300\n",
      "192/192 - 27s - loss: 0.0230 - val_loss: 1.5505 - lr: 0.0018 - 27s/epoch - 139ms/step\n",
      "Epoch 34/300\n",
      "192/192 - 27s - loss: 0.0211 - val_loss: 1.6219 - lr: 0.0018 - 27s/epoch - 139ms/step\n",
      "Epoch 35/300\n",
      "192/192 - 27s - loss: 0.0218 - val_loss: 1.5239 - lr: 0.0018 - 27s/epoch - 140ms/step\n",
      "Epoch 36/300\n",
      "192/192 - 26s - loss: 0.0198 - val_loss: 1.6252 - lr: 8.8673e-04 - 26s/epoch - 137ms/step\n",
      "Epoch 37/300\n",
      "192/192 - 26s - loss: 0.0194 - val_loss: 1.6373 - lr: 8.8673e-04 - 26s/epoch - 137ms/step\n",
      "Epoch 38/300\n",
      "192/192 - 27s - loss: 0.0183 - val_loss: 1.7047 - lr: 8.8673e-04 - 27s/epoch - 139ms/step\n",
      "Epoch 39/300\n",
      "192/192 - 27s - loss: 0.0181 - val_loss: 1.7300 - lr: 8.8673e-04 - 27s/epoch - 140ms/step\n",
      "Epoch 40/300\n",
      "192/192 - 27s - loss: 0.0188 - val_loss: 1.7305 - lr: 8.8673e-04 - 27s/epoch - 138ms/step\n",
      "Epoch 41/300\n",
      "192/192 - 26s - loss: 0.0186 - val_loss: 1.7448 - lr: 8.8673e-04 - 26s/epoch - 138ms/step\n",
      "Epoch 42/300\n",
      "192/192 - 27s - loss: 0.0175 - val_loss: 1.7683 - lr: 8.8673e-04 - 27s/epoch - 139ms/step\n",
      "Epoch 43/300\n",
      "192/192 - 26s - loss: 0.0179 - val_loss: 1.7854 - lr: 8.8673e-04 - 26s/epoch - 137ms/step\n",
      "Epoch 44/300\n",
      "192/192 - 26s - loss: 0.0177 - val_loss: 1.7572 - lr: 8.8673e-04 - 26s/epoch - 138ms/step\n",
      "Epoch 45/300\n",
      "192/192 - 27s - loss: 0.0168 - val_loss: 1.8282 - lr: 8.8673e-04 - 27s/epoch - 139ms/step\n",
      "Epoch 46/300\n",
      "192/192 - 26s - loss: 0.0163 - val_loss: 1.7673 - lr: 4.4336e-04 - 26s/epoch - 136ms/step\n",
      "Epoch 47/300\n",
      "192/192 - 26s - loss: 0.0149 - val_loss: 1.8398 - lr: 4.4336e-04 - 26s/epoch - 135ms/step\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in datasets:\n",
    "    dataset_name_csv = res[dataset_name]\n",
    "\n",
    "    dataset_manager = DatasetManager(dataset_name)\n",
    "\n",
    "    data = dataset_manager.read_dataset('Original_data/'+dataset_name_csv+'.csv')\n",
    "\n",
    "    for cls_encoding in encoding:\n",
    "        for level in incomplete_levels: \n",
    "\n",
    "            flip_ratio_ = levels[level]  \n",
    "            label_freq_ = 1.0 - flip_ratio_  ## P(labeled | y = 1)\n",
    "\n",
    "\n",
    "            print('Dataset:', dataset_name)\n",
    "            print('Classifier', cls_method)\n",
    "            print('Encoding', cls_encoding)\n",
    "            method_name = \"%s_%s\"%(column_selection, cls_encoding)\n",
    "            \n",
    "            #optimal parameters (see hyperopt file)\n",
    "            optimal_params_filename = os.path.join(params_dir, \"PU_optimal_params_%s_%s_%s_%s.pickle\" % (cls_method, dataset_name, level, method_name))\n",
    "            if not os.path.isfile(optimal_params_filename) or os.path.getsize(optimal_params_filename) <= 0:\n",
    "                print('problem')\n",
    "            with open(optimal_params_filename, \"rb\") as fin:\n",
    "                args = pickle.load(fin)\n",
    "                print(args)\n",
    " \n",
    "            # read the data\n",
    "            dataset_manager = DatasetManager(dataset_name)\n",
    "\n",
    "            #if dataset_name in ['bpic2011_f1', 'bpic2011_f2', 'bpic2011_f3', 'bpic2011_f4','bpic2015_1_f2','bpic2015_2_f2','bpic2015_3_f2','bpic2015_4_f2','bpic2015_5_f2','sepsis_cases_1','sepsis_cases_2','sepsis_cases_4']:\n",
    "            #data['time:timestamp'] = pd.to_datetime(data['time:timestamp'])\n",
    "            #if dataset_name in ['bpic2012_accepted', 'bpic2012_cancelled', 'bpic2012_declined']:\n",
    "            #data['Complete Timestamp'] = pd.to_datetime(data['Complete Timestamp'])\n",
    "\n",
    "            cls_encoder_args = {'case_id_col': dataset_manager.case_id_col, \n",
    "                                'static_cat_cols': dataset_manager.static_cat_cols,\n",
    "                                'static_num_cols': dataset_manager.static_num_cols, \n",
    "                                'dynamic_cat_cols': dataset_manager.dynamic_cat_cols,\n",
    "                                'dynamic_num_cols': dataset_manager.dynamic_num_cols, \n",
    "                                'fillna': True}\n",
    "                \n",
    "            #file to save results\n",
    "            outfile = os.path.join('', \"PU_performance_results_%s_%s_%s_%s.csv\" % (cls_method, dataset_name, level, method_name))\n",
    "                \n",
    "            # determine min and max (truncated) prefix lengths\n",
    "            min_prefix_length = 1\n",
    "            if \"traffic_fines\" in dataset_name:\n",
    "                max_prefix_length = 10\n",
    "            elif \"bpic2017\" in dataset_name:\n",
    "                max_prefix_length = min(20, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n",
    "            else:\n",
    "                max_prefix_length = min(40, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n",
    "            maxlen = cutoff = max_prefix_length\n",
    "                \n",
    "            # split into training and test\n",
    "            #train, test = dataset_manager.split_data_strict(data, train_ratio, split=\"temporal\")\n",
    "            train = dataset_manager.read_dataset('Data/Train_PU'+level+'_'+dataset_name_csv+'.csv')\n",
    "\n",
    "        \n",
    "            test = dataset_manager.read_dataset('Data/Test_'+dataset_name_csv+'.csv')\n",
    "\n",
    "                \n",
    "            #prefix generation of train and test data\n",
    "            dt_train_prefixes = dataset_manager.generate_prefix_data(train, min_prefix_length, max_prefix_length)\n",
    "            dt_test_prefixes = dataset_manager.generate_prefix_data(test, min_prefix_length, max_prefix_length)\n",
    "            \n",
    "            #transform data (padded)\n",
    "            cat_cols = cls_encoder_args['dynamic_cat_cols']+cls_encoder_args['static_cat_cols']\n",
    "            numerical_columns = cls_encoder_args['dynamic_num_cols']+cls_encoder_args['static_num_cols']\n",
    "            pad_train, pad_test, paddings_train, paddings_test, train_y, test_y = create_data(dt_train_prefixes, dt_test_prefixes, cat_cols,numerical_columns)\n",
    "\n",
    "            #DELETE THIS LATER\n",
    "            count_labels_number(train_y)\n",
    "            count_labels_number(test_y)\n",
    "        \n",
    "            #create the input layers and embeddings\n",
    "            embeddings= []\n",
    "            input_layers = []\n",
    "            preds_all = []\n",
    "            nr_events_all = []\n",
    "            nr_events = list(dataset_manager.get_prefix_lengths(dt_test_prefixes))\n",
    "            nr_events_all.extend(nr_events)\n",
    "            test_y_all = []\n",
    "            test_y_all.extend(test_y)\n",
    "            \n",
    "            score = 0\n",
    "            dim = 0\n",
    "            \n",
    "            #cat cols\n",
    "            for i in cat_cols:\n",
    "                no_values = len(data.groupby([i]))\n",
    "                cat_weights, index_cat, cat_index = create_indexes(i, data)\n",
    "                i=i.replace('(','_')               \n",
    "                i=i.replace(')','_')                \n",
    "                i=i.replace(' ','_')                \n",
    "                i=i.replace(':','_')\n",
    "                input_layer = Input(shape=(cutoff,), name=i)\n",
    "                embedding = Embedding(cat_weights.shape[0],\n",
    "                                      cat_weights.shape[1],\n",
    "                                      weights=[cat_weights],\n",
    "                                      input_length=no_values+1,\n",
    "                                      name='embed_'+i)(input_layer) \n",
    "                embeddings.append(embedding)\n",
    "                input_layers.append(input_layer)\n",
    "                dim += cat_weights.shape[1]\n",
    "\n",
    "            #static input layers\n",
    "            for j in numerical_columns:\n",
    "                j=j.replace('(','_')               \n",
    "                j=j.replace(')','_')                \n",
    "                j=j.replace(' ','_')                \n",
    "                j=j.replace(':','_')\n",
    "                input_layer = Input(shape=(cutoff,1), name=j)\n",
    "                input_layers.append(input_layer)\n",
    "                embeddings.append(input_layer)\n",
    "                dim +=1\n",
    "\n",
    "            #create the model inputs\n",
    "            model_inputs= []\n",
    "            model_inputs_test= []\n",
    "            for i in range(0,len(paddings_train)):\n",
    "                 model_inputs.append(paddings_train[i])\n",
    "\n",
    "            for i in range(0,len(paddings_test)):\n",
    "                model_inputs_test.append(paddings_test[i])\n",
    "\n",
    "            for i in range(0,len(pad_train)):\n",
    "                model_inputs.append(reshape_num_data(pad_train[i], cutoff))\n",
    "\n",
    "            for i in range(0,len(pad_test)):\n",
    "                model_inputs_test.append(reshape_num_data(pad_test[i], cutoff))\n",
    "\n",
    "            full_embs = concatenate(embeddings, name='full_embedding')\n",
    "            l2reg=0.001\n",
    "            # train a 2-layer bidirectional LSTM with dropout\n",
    "            l1 = Bidirectional(LSTM(args['lstm_size'], return_sequences=True,  dropout=args['LSTM_dropout']), name='LSTM1')\n",
    "            l1_out = l1(full_embs)\n",
    "            l2 = Bidirectional(LSTM(args['lstm_size'], return_sequences=False, dropout=args['LSTM_dropout']), name='LSTM2')\n",
    "            l2_out = l2(l1_out)      \n",
    "            output_layer = Dense(1, activation='sigmoid', name='final_output')(l2_out)\n",
    "            \n",
    "            #MODEL\n",
    "            model = Model(inputs=[input_layers], outputs=output_layer)\n",
    "\n",
    "            if args['optimizer']=='RMSprop':\n",
    "                opt = RMSprop(learning_rate=args['learning_rate'])\n",
    "            if args['optimizer']=='Nadam':\n",
    "                opt = Nadam(learning_rate=args['learning_rate'])\n",
    "            if args['optimizer']=='Adam':\n",
    "                opt = Adam(learning_rate=args['learning_rate'])\n",
    "            if args['optimizer']=='SGD':\n",
    "                opt = SGD(learning_rate=args['learning_rate'])\n",
    "\n",
    "            model.compile(loss={'final_output':puk.nnPU_loss(label_freq_)}, optimizer= opt)\n",
    "\n",
    "            model.summary()\n",
    "\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=42)\n",
    "            model_checkpoint = ModelCheckpoint('output_files/models/model_{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "            lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "            result = model.fit(model_inputs,\n",
    "                               np.array(train_y),\n",
    "                               callbacks=[early_stopping, lr_reducer],\n",
    "                               validation_split = 0.1,\n",
    "                               verbose=2, batch_size=args['batch_size'],\n",
    "                               epochs=300) \n",
    "            pred = model.predict(model_inputs_test)\n",
    "            preds_all.extend(pred)\n",
    "            auc_total = roc_auc_score(test_y_all, preds_all)\n",
    "\n",
    "            outfile = os.path.join(results_dir, \"PU_performance_results_%s_%s_%s_%s.csv\" % (cls_method, dataset_name, level, method_name))\n",
    "\n",
    "            with open(outfile, 'w') as fout:\n",
    "                fout.write(\"%s;%s;%s;%s;%s;%s;%s\\n\"%(\"dataset\",\"level\", \"method\", \"cls\", \"nr_events\", \"metric\", \"score\"))\n",
    "                dt_results = pd.DataFrame({\"actual\": test_y_all, \"predicted\": preds_all, \"nr_events\": nr_events_all})\n",
    "                for nr_events, group in dt_results.groupby(\"nr_events\"):\n",
    "                    if len(set(group.actual)) < 2:\n",
    "                        fout.write(\"%s;%s;%s;%s;%s;%s;%s;%s\\n\"%(dataset_name,level, method_name, cls_method, nr_events,-1, \"auc\", np.nan))\n",
    "                    else:\n",
    "                        fout.write(\"%s;%s;%s;%s;%s;%s;%s;%s\\n\"%(dataset_name,level, method_name, cls_method, nr_events,-1, \"auc\", roc_auc_score(group.actual, group.predicted)))\n",
    "                fout.write(\"%s;%s;%s;%s;%s;%s;%s\\n\"%(dataset_name,level, method_name, cls_method,-1, \"auc\", roc_auc_score(dt_results.actual, dt_results.predicted)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RFhRU16LwdC3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "v0yRQ_ZSMaFG",
    "UpCBthWhMhkS",
    "HcKLuejWcyxN"
   ],
   "machine_shape": "hm",
   "name": "uPU_Experiments_DL (PU-OOPPM).ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
